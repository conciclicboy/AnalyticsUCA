---
title: "Métodos de Regresión"
author: "Eduardo Aguilar (inspirado en INSEADAnalytics)"
output:
  html_document:
    css: default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  #pdf_document:
    #includes:
      #in_header: /AnalyticsStyles/default.sty
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## AirBnB Pricing Tool
**Este caso está basado en el curso de analítica aplicada de INSEAD. Puede encontrar la solución original en el archivo [AirBnB Pricing Tool](http://inseaddataanalytics.github.io/INSEADAnalytics/groupprojects/January2018FBL/Airbnb_Pricing_TeamR_MASTER.HTML). Las modificaciones y soluciones alternativas son responsabilidad de Eduardo Aguilar**


## El contexto de negocios
AirBnB fue fundada en 2008 por Brian Cheski, Joe Gebbia y Nathan Blecharczyk como AirBed & Breakfast, un sitio de ventas online para servicios de hospedajes de corto tiempo que incluía personas comunes y profesionales dedicados a ese negocio. Con el paso de los años, el porcentaje de personas que se dedican profesionalmente a recibir huéspedes ha incrementado, y está comenzando a desplazar a las personas comunes que ponen sus propiedades en alquiler, amenazando la naturaleza y propuesta de valor de AirBnB que ofrecía servicios económicos y personalizados. En este contexto, el departamento de marketing desea iniciar una campaña para atraer más anfitriones privados. Para hacer esto, ellos solicitaron al departamento de analítica crear un modelo que ayudara a entender a anfitriones potenciales cuanto dinero podrían obtener por poner su propiedad a disposición de AirBnB.

Como plan piloto, Amsterdam fue elegida como una ciudad modelo con el servicio debido a su regulación "amigable" con AirBnB y alto número de clientes de estancias cortas. La solución presentada, sin embargo, está diseñada para ser independiente de la ciudad y por lo tanto, un proceso replicable en otras ciudades.


## Los datos
La base de datos fue extraida del sitio http://tomslee.net/airbnb-data-collection-get-the-data, con la carpeta correspondiente a la ciudad de Amsterdam.
Estos fueron recolectados de la página oficial de AirBnB por Tom Slee. La base contiene 18723 observaciones con 15 variables.


Nombre                     | Descripción
:--------------------------|:--------------------------------------------------------------------
room_id	                   | ID de la habitación en alquiler
survey_id                  | ID de la consulta
host_id                    | ID del anfitrión
room_type                  | Casa completa, habitación privada o habitación compartida
city                       | Ciudad
neighborhood               | Subregión de la ciudad en la que se hace la consulta (Bijlmer Centrum, Bijlmer Oost, Bos en Lommer, Buitenveldert / Zuidas, Centrum Oost, Centrum West, De Aker / Nieuw Sloten, De Baarsjes / Oud West, De Pijp / Rivierenbuurt, Gaasperdam / Driemond, Geuzenveld / Slotermeer, Ijburg / Eiland Zeeburg, Noord-West / Noord-Midden, Noord Oost, Noord West, Oostelijk Havengebied / Indische Buurt, Osdorp, Oud Noord, Oud Oost, Slotervaart, Watergraafsmeer, Westerpark, Westpoort)
reviews                    | Número de revisiones que ha recibido. Como 70% de los huéspedes dan una revisión, este puede ser usado para estimar el número de visitas. Note que este estimado no será confiable para una lista lista pequeña, pero tomada en una ciudad puede ser una métrica muy útil.
overall_satisfaction	     | Rating promedio que recibió la unidad de todos los reviews, en escala de 1 al 5. 
accommodates               | Número de personas que pueden ser acomodadas
bedrooms                   | Número de cuartos en la unidad
minstay                    | Estadía mínima
latitude                   | Ubicación geográfica
longitude                  | Ubicación geográfica
last_modified              | Fecha de consulta de los datos
price                      | Precio de la unidad

Leyendo los datos: completar el siguiente bloque de código

```{r, echo=TRUE}


# 1. Chequear donde está el directorio: getwd()


# 2. Leer el archivo tomslee_airbnb_amsterdam: read.csv()
#    Guardarlo con el nombre b
# b<- ...

# 3. Esto es solo para cambiar el formato: se guarda el Data.Frame b 
#    en la variable a como una matriz

#    DESCOMENTAR SIGUIENTE LÍNEA
#a<-data.matrix(b)



# 4. Imprimiento las primeras 3 filas: 
#    t significa transponer, 
#    head(a,3) significa primeras 3 filas de a
#    round(a,2) significa redondear los valores a 2 decimales

#    DESCOMENTAR SIGUIENTES LÍNEAS
#knitr::kable({
#  df <- t(head(round(a,2), 5))  
#  colnames(df) <- sprintf("%02d", 1:ncol(df))
#  df
#})


```


## Sacando estadísticas de las columnas
Vamos a ver que hay dentro de cada variable para decidir si vamos a botar algunas.

```{r, echo=TRUE}
# 1. Utilice la función "summary" con la tabla "b"

```


De acá es fácil determinar que algunas columnas no podremos utilizarlas para nuestro análisis, entonces simplemente vamos a deshacernos de ellas. No siempre es recomendado hacer esto, existen criterios de imputación de variables cuando se tienen datos faltantes.


## Eliminando filas innecesarias
Ya que hay columnas que están llenas de campos vacíos o extraños las vamos a eliminar para no tomarlas en el análisis

```{r, echo=TRUE}

#1. Elimine las variables que tienen valores NA
#   Simplemente se asigna NULL a la variable que se desea eliminar
#   Por ejemplo si quisiera eliminar var1 haría esto: b$var1 <- NULL


```

Después de limpiar deberían de quedar menos variables:

## Explorando las variables

### Distribución de precios
Si se quiere pronosticar precios, lo primero que se debe tener es una idea de como están distribuidos los mismos. El método más común es haciendo un histograma de la variable:


```{r,message=FALSE,warning=FALSE, echo=TRUE}

# 1. Llame la librería ggplot2

# 2. Cree un objeto con la función ggplot()
#    este debe recibir el DATAFRAME b y la VARIABLE price
#pl <- ggplot(DATAFRAME, aes(x=VARIABLE_A_EXPLORAR))

# 3. Grafique el histograma
#    DESCOMENTAR LA SIGUIENTE LINEA
#pl+geom_histogram(fill='blue',alpha=0.5)+theme_minimal()

```


El problema con el histograma anterior es que hay demasiados outliers o valores extremos. Así se vuelve muy difícil visualizar cualquier tipo de correlación, y además, como muchos de estos modelos tienen soluciones numéricas, la convergencia de los mismos se vuelve extraña. Lo que vamos a hacer es tomar los datos entre los percentiles 1 y 99 para la variable precio.

```{r,message=FALSE,warning=FALSE, echo=TRUE}

# 1. Filtre los valores mayores o iguales a percentil 1
#    La función quantile(DATO$VAR, p) sirve para determinar
#    el percentil p, de la variable VAR en el dataframe DATOS.
#    DESCOMENTAR LA SIGUIENTE LINEA
#b<- b [b$price>=quantile(b$price,0.01),]


# 2. Filtre los valores menores o iguales a percentil 99


# 3. Grafique nuevamente el histograma de la variable price.


```


Este gráfico se ve mucho mejor, se puede observar que la mayor parte de los precios se ubican entre $100 y $200.


### Diagramas de dispersión
Cuando se trata de un problema de regresión o pronóstico, lo más apropiado es hacer un diagrama de dispersión. para verificar si los datos se distribuyen linealmente con respecto a alguna variable o ver algún tipo de correlación visual. Puede parecer demasiado empírico, y en efecto, estas metodologías son de naturaleza empírica, así que manos a la obra. Dado que algunas variables son categóricas, primero procederemos a crear variables dummy.


```{r, message=FALSE, warning=FALSE, echo=TRUE }

# Para no hacernos la vida imposible podemos hacer muchos scatter plot
# utilizando la función visualize de la librería radiant.data


# 1. Llamar la librería radiant.data (instalarla en caso lo necesite)


# 2. Utilizar la función visualize para ver los diagramas de dispersión.
#    COMPLETE EL SIGUIENTE BLOQUE DE CÓDIGO

#visualize(dataset= NOMBRE DEL DATAFRAME,
#          xvar=c("VARIABLES INDEPENDIENTES""),
#          yvar="VARIABLE DEPENDIENTE",
#          type="scatter",
#          custom=FALSE
#)

```



### Diagrama de caja de los vecindarios
Con los vecindarios dibujaremos individualmente el diagrama ya que son muchos. En este caso usaremos un boxplot porque es una visualización más para este tipo de variables categóricas.

```{r, echo=FALSE}
pl<-ggplot(data=b,aes(y=price,x=neighborhood))
pl+geom_boxplot(fill='blue',alpha=0.5)+theme_minimal()+theme(axis.text.x=element_text(angle=90, hjust=1))
```

Pero debido a que son una variable categórica, debemos intentar incluir solo a los vecindarios que sean estadísticamente relevantes dentro de nuestra regresión. Nos vamos a deshacer de los vecindarios que tengan menos de x% de elementos


```{r, echo=TRUE }

# 1. Defina el límite inferior de los vecindarios a considerar
#    como un 4% del total de datos en nuestra base (nrow)
#    COMPLETE LA SIGUIENTE LINEA DE CODIGO
#lim_inferior<- #...


# 2. Vamos a graficar el número de elementos por vecindario
#    DESCOMENTE EL BLOQUE DE CODIGO
#pl<-ggplot(data=b,aes(x=neighborhood))
#pl+geom_bar(fill='blue',alpha=0.5)+theme_minimal()+theme(axis.text.x=element_text(angle=90, hjust=1)) +geom_hline(yintercept = lim_inferior)

```

### Conteo de variables categóricas
Ahora que tenemos un límite inferior para los vecindarios podemos volver a reducir el número de datos, limitandonos solo a los vecindarios con más del x% de elementos

```{r, echo=TRUE}
# 1. Crear una tabla c que contenga los elementos de b con vecindarios
#    con al menos 700 personas (se podría cambiar pero dejemos 700).
#    DESCOMENTAR LA SIGUIENTE LINEA DE CÓDIGO

#c<- b[b$neighborhood %in%  names(table(b$neighborhood))[table(b$neighborhood) >= 700],]


# 2. Vuelva a graficar el número de elementos por vecindario

```


Ahora, después de ver estos diagramas, tiene un poco más de sentido incluir dichos vecindarios como potenciales variables de nuestro modelo, pues el precio medio depende de estos.

## Separando datos
Suponga que usted toma todos los datos y crea un modelo con una precisión casi perfecta que le permitiría predecir el precio de los alquileres de airbnb. ¿Tomaría usted la decisión de implementarlo inmediatamente?
Por supuesto que es una decisión personal sin una respuesta correcta, pero en la práctica no se recomienda hacer eso. La razón es que todos los modelos deben ser validados estadísticamente para estar más seguros que su funcionamiento sea consistente en el tiempo.
Más adelante veremos a detalle esta técnica, pero en este momento, simplemente dividiremos los datos en dos grupos: uno con el 90% y otro con el 10%.

Antes de hacer eso, tenemos que idear una manera de representar variables categóricas (cualitativas) como variables cuantitativas, la práctica más común es hacerlo con la técnica llamada One Hot encoding, que es, crear tantas variables como diferentes valores tenga una variable cuantitativa, las cuales tomarán el valor de 1 únicamente en la variable correspondiente al valor original y cero en las demás.
Los modelos base de R como linear model (lm) pueden lidiar con variables categóricas fácilmente. Sin embargo, otro tipo de modelos no tiene esa funcionalidad, así que implementaremos esto desde antes:

```{r, echo=FALSE}
# One Hot Encoding

# 1. Columnas de Neighborhood
vecindarios<-model.matrix(~neighborhood-1,b)

# 2. Columnas de Room_type
tipos_cuarto<-model.matrix(~room_type-1,b)

# 3. Uniendo tablas
b <- cbind(b, vecindarios,tipos_cuarto)
```


El código de arriba habrá agregado nuevas columnas al dataframe b. Estas son algunas de las columnas que hemos añadido:

```{r, echo=FALSE}
#length(colnames(b))
colnames(b)[40:45]
```


Ahora que ya tenemos codificadas las variables categóricas podemos proceder a separar los datos:

```{r, echo=FALSE}

# 1. Vamos a dividir en datos de entrenamiento 90%, datos de evaluación 10%
porcion_train = 0.9
porcion_test = 0.1

# Importando librería dplyr
library(dplyr)

# Creando índice con función "mutate"" de dplyr
# Esto lo utilizaremos solo como referencia
b <- mutate(b,id = row_number())

# 2. Conjunto de entrenamiento
# Creando datos de entrenamiento con función "sample_frac" de dplyr
# La función toma una muestra aleatoria de tamaño "porcion_train" de "b"
train_set <- sample_frac(b,porcion_train)

# 3. Conjunto de evaluación
# Creando datos de evaluación con función "anti_join" de dplyr
# La función extraen los datos de "b" que no coinciden con "train_set"
# usando como referencia la columna "id"
test_set <- anti_join(b, train_set, by = 'id')
```

Al tener listos los datos, podemos pasar a crear nuestro modelo con los datos de entrenamiento (train_set).

## Modelo
Vamos a construir varios modelos y seleccionaremos el que funcione mejor.
Nuevamente, eso puede sonar a que lo estamos a pura haciendo prueba y error, y en efecto esto así es, usted raramente encontrará el mejor modelo a la primera iteración. Tampoco encontraremos el mejor modelo en esta sesión, pero pretenedemos acercarnos a una solución subóptima que sea mejor que una solución aleatoria.


### Modelo 1: Regresión Lineal Clásica
Tomaremos como variables independientes room_type, neighborhood, reviews, overall_satisfaction, accommodates, bedrooms y haremos una regresión lineal respecto al precio. El siguiente bloque de código ajusta el modelo para solamente 3 variables independientes (room_type, neighborhood y reviews) y una dependiente (overall_satisfaction), usted deberá ajustarlo para las variables faltantes:

```{r,message=FALSE, warning=TRUE, echo=TRUE, eval=FALSE}

# 1. Complete el modelo con variables independientes room_type, 
#   neighborhood, reviews, overall_satisfaction, accommodates, bedrooms
#   y variable dependiente price.
model_1<- lm(formula = overall_satisfaction ~ room_type + neighborhood + 
             reviews,
             data = train_set)


# 2. Sacando predicciones para los datos de evaluación
pred_train_1=predict(model_1,train_set)#entrenamiento
pred_test_1=predict(model_1,test_set)#evaluación


# 3. Reportando modelo en formato apropiado: la función "summary(model)"
#    entrega los resultados del modelo pero se imprime en formato feo
#    Nosotros lo vamos a imprimir como una "xtable" en "knitr::kable"

library(xtable)
knitr::kable(xtable(summary(model_1)))

```


La mayoría de variables son significativas con p-values pequeños.

### Evaluando los modelos
Luego de ajustar el/los modelo/s debemos evaluar el desempeño de los mismos. Para el caso de la regresión, la métrica a utilizar es llamada MAE (Mean Absolute Error) que en inglés significa Error Absoluto Medio. Estos son los resultados que deberíamos obtener:

```{r,message=FALSE, warning=FALSE, echo=TRUE }
# Evaluando: Error Absoluto Medio
library(Metrics)
mae1<-mae(test_set$price,pred_test_1)
print((paste("Modelo 1:", mae1)))

```


### Modelo 2: Regresión log-lineal

Tomaremos como variables independientes room_type, neighborhood, reviews, overall_satisfaction, log(accommodates), bedrooms y haremos una regresión lineal respecto a log(precio). Luego, al hacer la predicción, recuperaremos el precio original haciendo exp(predicción) ya que está en escala logarítmica.
Modifique el modelo:

```{r,message=FALSE, warning=TRUE, echo=TRUE, eval=FALSE}

# 1. Complete el modelo con variables independientes room_type, 
#    neighborhood, reviews, overall_satisfaction, log(accommodates), bedrooms
#    y variable dependiente log(price).

#model_2<-lm(formula = ...)

# 2. Sacando predicciones
#pred_train_2=exp(predict(model_2,train_set))
#pred_test_2=exp(predict(model_2,test_set))

# 3. Reportando modelo
#knitr::kable(xtable(summary(model_2)))

```


Mientras que el error absoluto medio debería ser cercano a:

```{r,message=FALSE, warning=FALSE, echo=FALSE }
# Evaluando: Error Absoluto Medio
mae2<-mae(test_set$price,pred_test_2)
print((paste("Modelo 2:", mae2)))

```

### Modelo 3: Arbol de regresión
En este caso estaríamos usando las mismas variables predictivas que en el modelo 1 pero con un árbol de regresión. Un árbol de regresión, al no ser un modelo paramétrico basado en supuestos probabilísticos, no debe ser evaluado de la misma forma, así que presentaremos los resultados después.


```{r,message=FALSE, warning=TRUE, echo=TRUE, eval=FALSE}

# 1. Complete el modelo con variables independientes room_type, 
#    neighborhood, reviews, overall_satisfaction, accommodates, bedrooms
#    y variable dependiente price.

#library(rpart)
#model_3 <- rpart(reviews ~ room_type + accommodates +
#             bedrooms,data = train_set, method="anova")

# 2. Sacando predicciones
#pred_train_3=predict(model_3,train_set)#entrenamiento
#pred_test_3=predict(model_3,test_set)#evaluación
```

En este casi la regresión con árboles no funcionó tan bien, sin embargo podemos intentar con métodos aún más poderosos.


### Modelo 4: XGBoost

XGBoost es el nombre del paquete que efectua el algoritmo de Gradient Boosting Decision Tree, el cual es un árbol que se va desarrollando por etapas, y en cada etapa mejora usando como insumo el error de la etapa anterior. Como XGBoost es otro método basado en árboles, por lo tanto no hace ningún supuesto probabilístico, y tampoco es necesario evaluar el ajuste de la misma forma que con la regresión lineal.

```{r,message=FALSE, warning=FALSE, echo=TRUE, eval=FALSE}
library(xgboost)

# XGBoost, a diferencia de lm, no convierte las variables categóricas que aparecen como
# caracteres a variables binarias (0/1). Tendremos que usar la codificación

# 1. Primero vamos a definir las matrices de entrenamiento y evaluación
#    Incluya los siguientes campos: todos los vecindarios y tipos_cuarto,
#    reviews, overall_satisfaction, accommodates, bedrooms,
#    longitude, latitude

X_train<- train_set[,c(colnames(vecindarios),
                       "overall_satisfaction",
                       "latitude")]

X_test<- test_set[,c(colnames(vecindarios),
                       "overall_satisfaction",
                       "latitude")]

# 2. Luego definimos los vectores de la variable dependiente
#y_train<- train_set$...
#y_test<-test_set$...

# 3. Definiendo hiperparámetros del modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4, 
                   eta = 0.3, nthread = 2, nrounds = 50, objective = "reg:linear",
                   eval_metric="rmse",verbose = 1)

# 4. Calculando predicciones: complete pred_test
pred_train_4<-predict(model_4,as.matrix(X_train))
#pred_test_4<-predict(...)

```


Y al evaluar el modelo obtenemos:

```{r,message=FALSE, warning=FALSE, echo=FALSE }
# Evaluando: Error Absoluto Medio
mae4<-mae(test_set$price,pred_test_4)
print((paste("Modelo 2:", mae4)))

```


## Comparando modelos
Se debe seleccionar un modelo a partir de sus resultados. En el caso de los 4 modelos anteriores nosotros habíamos utilizado la métrica MAE (Mean Absolute Error) que en inglés significa Error Absoluto Medio.

Ciertamente para el caso de la regresión lineal se deben evaluar muchos otros factores para asegurar la significancia estadística del modelo como normalidad, homecedasticidad, etc. Sin embargo, a la hora de comparar un modelo lineal con uno no lineal, no se pueden utilizar todos estos factores ya que no son parte de los supuestos. Es por eso que nos enfocaremos solamente en la desviación absoluta media. Retomando los valores que obtuvimos, los resultados habían sido estos:

```{r,message=FALSE, warning=FALSE, echo=FALSE}

#Librería de métricas
#install.packages("Metrics")
library(Metrics)
mae1<-mae(test_set$price,pred_test_1)
mae2<-mae(test_set$price,pred_test_2)
mae3<-mae(test_set$price,pred_test_3)
mae4<-mae(test_set$price,pred_test_4)

# Mostrando resultados
print((paste("Modelo 1:", mae1)))
print((paste("Modelo 2:", mae2)))
print((paste("Modelo 3:", mae3)))
print((paste("Modelo 4:", mae4)))

```

Así que hasta el momento, el mejor modelo es el Modelo #4.

## Modelo 5: Ensamblando modelos 1 y 2 con los datos
Una técnica común en ciencia de datos es ensamblar modelos. La idea es seleccionar el resultado de modelos diferentes y construir en base a ellos un modelo que tenga un desempeño superior. Podemos construir una nueva base de datos que contenga las predicciones del modelo 1, las predicciones del modelo 2, así como las variables independientes originales: room_type, neighborhood, reviews, overall_satisfaction, accommodates, bedrooms. El resultado final de este modelo usualmente es superior a los modelos anteriores:


```{r, warning=FALSE, message=FALSE, echo=TRUE}

# Primero vamos a añadir a X_train y X_test los resultados del modelo lineal
y_1<- as.matrix(pred_train_1)
y_2<- as.matrix(pred_train_2)
X_train_2<-cbind(X_train,y_1,y_2)

y_1<- as.matrix(pred_test_1)
y_2<- as.matrix(pred_test_2)
X_test_2<-cbind(X_test, y_1,y_2)

# 1. Modelo: Aplique xgboost a los datos
# model_5 <- xgboost(...)

# 2. Saque predicciones
# pred_train_5<-predict(...)
# pred_test_5<-predict(...)

# 3. Calcule Error Medio (MAE)
#mae5<-mae(...)

# 4. Mostrando Resultados
#print((paste("Modelo 5:", mae5)))

```


## Conclusiones
* Si se tienen modelos con resultados diferentes, es posible construir un modelo final que sea capaz de superar modelos anteriores simplemente ensamblando los resultados y aplicando XGboost.

* Las propiedades de homocedasticidad y pvalues son importantes mientras se considere únicamente modelos lineales que asuman normalidad, para modelos de regresión que no sean lineales la métrica de comparación más apropiada sería la desviación absoluta media o la desviación cuadrática media.

* Para mejorar los resultados debemos mejorar el análisis exploratorio, construir nuevas variables o mejorar los datos iniciales.

> Analítica de datos es un proceso iterativo, podríamos necesitar regresar a nuestros datos originales en cualquier momento y seleccionar nuevos atributos, así como diferentes herramientas de regresión y modelamiento.