---
title: "Clasificación en Pago de Tarjetas de Crédito"
author: "Theos Evgeniou, Spyros Zoumpoulis. Traducción por Eduardo Aguilar, UCA"
output:
  html_document:
    css: ../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../AnalyticsStyles/default.sty
always_allow_html: yes
---

**Este caso ha sido traducido a español del curso de analítica aplicada de INSEAD. Puede encontrar la solución original en el archivo [Classification for Credit Default](http://inseaddataanalytics.github.io/INSEADAnalytics/CourseSessions/ClassificationProcessCreditCardDefault.html) o en el archivo [Classification for Credit Default More Methods](http://inseaddataanalytics.github.io/INSEADAnalytics/CourseSessions/ClassificationProcessCreditCardDefaultMoreMethods.html). El código original ha sufrido pocas modificaciones.**



```{r echo=FALSE, message=FALSE}
make_pdf_file = 0 # Haga este número 1 si quiere un archivo PDF, 0 para HTML

print(getwd())
source("../../AnalyticsLibraries/library.R")
source("../../AnalyticsLibraries/heatmapOutput.R")

# Opciones de paquetes
ggthemr('fresh')  # tema de ggplot
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.2)
options(knitr.kable.NA = '')
```

```{r echo=FALSE, message=FALSE}
# Por favor INGRESE el nombre del archivo
datafile_name = "../../Data/UCI_Credit_Card.csv"
ProjectData <- read.csv(datafile_name)
# Convertimos los datos a la clase data.matrix para que sea mas fácil de manipular
ProjectData <- data.matrix(ProjectData)

# Por favor INGRESE la variable dependiente(clase).
# Por favor use números, no nombres de columnas. Por ejemplo, 82 usa la columna en la posición 82 como la variable dependiente.
# Necesita asegurarse que la variable dependiente solo tome dos valores: 0 y 1.
dependent_variable = 25

# Por favor ingrese los atributos a usar como variables independientes.
# Por favor use números, no nombres de columnas. Por ejemplo, c(1:5, 7, 8) usa las columnas 1,2,3,4,5,7,8.
independent_variables = c(1:24) # usa todos los atributos disponibles

dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectData), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectData), max(i,1))))

if (length(unique(ProjectData[,dependent_variable])) !=2){
  cat("\n*****\n REVISE DE NUEVO, LA VARIABLE DEPENDIENTE TOMA MÁS DE 2 VALORES")
  cat("\nDividiendo los datos respecto a la mediana...\n*****\n ")
  new_dependent = ProjectData[,dependent_variable] >= median(ProjectData[,dependent_variable])
  ProjectData[,dependent_variable] <- 1*new_dependent
}

# Por favor INGRESE el umbral de probabilidad arriba del cual una observación será categorizada como clase 1:
Probability_Threshold = 0.5 # entre 0 y 1

# Por favor INGRESE el porcentaje de datos usados para la estimación
estimation_data_percent = 80
validation_data_percent = 10
test_data_percent = 100-estimation_data_percent-validation_data_percent

# Por favor INGRESE 1 si quiere dividir los datos aleatoriamente en los conjuntos de estimación y validación/evaluación
random_sampling = 0

# Parámetros del árbol
# Por favor INGRESE el control de complejidad cp del árbol(CART), por ejemplo de 0.0001 a 0.02, dependiendo de los datos.
CART_cp = 0.0025
CART_control = rpart.control(cp = CART_cp)

# Por favor INGRESE el significado de las clases 1 y 0:
class_1_interpretation = "default"
class_0_interpretation = "no default"

# Por favor INGRESE los valores de ganancia/costo de clasificar correcta o incorrectamente los datos:
actual_1_predict_1 = 0
actual_1_predict_0 = -100000
actual_0_predict_1 = 0
actual_0_predict_0 = 20000

Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix) <- c(paste("Predict 1 (", class_1_interpretation, ")", sep = ""), paste("Predict 0 (", class_0_interpretation, ")", sep = ""))
rownames(Profit_Matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))

# Por favor INGRESE el máximo número de observaciones a mostrar en el reporte y diapositivas 
# (El número predefinido es 50. Si el número es demasiado grande el reporte y las diapositivas podrían no ser generados -  muy lento o no funcionará!!)
max_data_report = 10 
```

# El Contexto de Negocios
Un emisor de tarjetas de crédito de Taiwan quiere mejorar la predicción de la probabilidad de impago de sus clientes, así como identificar las carcaterísticas clave que determinan esta probabilidad. Esto informaría al emisor acerca de a quien dar una tarjeta así como que límite dar. También ayudaría  al emisor a tener un mejor entendimiento de sus actuales y potenciales consumidores, lo cuál afectaría su futura estrategia, incluyendo sus planes de ofrecer productos crediticios específicos a sus consumidores.


<hr>\clearpage

# Los Datos
(Fuente: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset . Reconocemos lo siguiente:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.)

El emisor de tarjetas de crédito ha reunido información de `r nrow(ProjectData)` consumidores. Los datos contienen información de `r length(independent_variables)` variables, incluyendo factores demográficos, datos de crédito, historial de pagos, y estados de cuenta de tarjetas de crédito desde Abril 2005 a Septiembre 2005, así como información del resultado: El cliente pagó o no?

Nombre                       | Descripción
:--------------------------|:--------------------------------------------------------------------
ID                         | ID de cada cliente
LIMIT_BAL                  | Monto del crédito en dólares (incluyendo créditos individuales y familiares/suplementarios)
SEX                        | Género (1=masculino, 2=femenino)
EDUCATION                  | (1=postgrado, 2=universidad, 3=bachillerato, 4=otros, 5=desconocido, 6=desconocido)
MARRIAGE                   | Estatus Marital (1=casado, 2=soltero, 3=otros)
AGE                        | Edad en años
PAY_0                      | Estatus de pago en Septiembre 2005 (-2=no consumo, -1=paga debidamente, 0=usa crédito rotativo, 1=atraso de un mes, 2=atraso de dos meses, ... 8=atraso de ocho meses, 9=atraso de nueve meses y más)
PAY_2                      | Estatus de pago en Agosto 2005 (misma escala)
PAY_3                      | Estatus de pago en Julio 2005 (misma escala)
PAY_4                      | Estatus de pago en Junio 2005 (misma escala)
PAY_5                      | Estatus de pago en Mayo 2005 (misma escala)
PAY_6                      | Estatus de pago en Abril 2005 (misma escala)
BILL_AMT1                  | Monto a pagar en Septiembre 2005 (dólares)
BILL_AMT2                  | Monto a pagar en Agosto 2005 (dólares)
BILL_AMT3                  | Monto a pagar en Julio 2005 (dólares)
BILL_AMT4                  | Monto a pagar en Junio 2005 (dólares)
BILL_AMT5                  | Monto a pagar en Mayo 2005 (dólares)
BILL_AMT6                  | Monto a pagar en Abril 2005 (dólares)
PAY_AMT1                   | Monto de pago anterior en Septiembre 2005 ( dólares)
PAY_AMT2                   | Monto de pago anterior en Agosto 2005 ( dólares)
PAY_AMT3                   | Monto de pago anterior en Julio 2005 ( dólares)
PAY_AMT4                   | Monto de pago anterior en Junio 2005 ( dólares)
PAY_AMT5                   | Monto de pago anterior en Mayo 2005 ( dólares)
PAY_AMT6                   | Monto de pago anterior en Abril 2005 ( dólares)
default.payment.next.month | Default (1=si, 0=no)

Veamos la información de unos pocos consumidores. Así es como las primeras `r min(max_data_report, nrow(ProjectData))` filas de un total de `r nrow(ProjectData)` se ven (transpuestas, por conveniencia):

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
knitr::kable({
  df <- t(head(round(ProjectData[,independent_variables],2), max_data_report))
  colnames(df) <- sprintf("%02d", 1:ncol(df))
  df
})
```

<hr>\clearpage


# Un proceso de Clasificación

> Es importante recordar que los proyectos de analítica requieren de un delicado balance entre experimentación e intuición, pero también siguiendo un proceso. El valor de seguir un proceso es para evitar ser engañado por la aleatoriedad en los datos y encontrar "patrones y resultados" que pueden estar explicados principalmente por nuestro propio sesgo y no por hechos/datos en sí. 
**No existe tal cosa cómo el mejor proceso** de clasificación. Sin embargo, tenemos que empezar por algo, así que haremos uso del siguiente proceso:

1. Crear un conjunto de estimación y dos conjuntos de evaluación dividiendo los datos en tres grupos. Los pasos 2-5 a continuación serán efectuados solo en el conjunto de estimación y el primer conjunto de validación. Usted debe efectuar el paso 6 una vez en el segundo conjunto de validación, llamado **datos de evaluación**, y reportar el desempeño en ese (segundo) conjunto sólo para tomar la decisión de negocios final.
2. Establecer la variable dependiente (como una variable categórica 0-1; clasificación múltiple también es posible, y similar, pero no exploraremos en eso en estos apuntes).
3. Hacer un diagnóstico preliminar acerca de la importancia relativa de las variables explicativas utilizando herramientas de visualización y simples estadísticas descriptivas.
4. Estimar el modelo de clasificación utilizando los datos de estimación, e interpretando los resultados.
5. Evaluar la precisión de la clasificación en el primer conjunto de validación, posiblemente repitiendo os pasos 2-5 unas cuantas veces en diferentes maneras para mejorar el desempeño.
6. Finalmente, evaluar la precisión de la clasificación en el segundo modelo de validación. Usted eventualmente usará/reportará todas las medidas y gráficas de desempeño solamente en el segundo conjunto de validación. 


Sigamos esos pasos.

## Paso 1: Dividir los datos
Es muy importante que usted mida y reporte (o espera ver de parte del científico de datos encargado del proyecto) el desempeño de su modelo en **datos que no han sido utilizados durante el análisis, estos son llamados "out-of-sample set", "test dataset" o "conjunto de evaluación"** (pasos 2-5). La idea es que en la práctica deseamos utilizar los modelos para predecir la clase de observaciones que no han sido observadas aún (por ejemplo "los datos futuros"): aunque el desempeño del método de clasificación puede ser alto en los datos utilizados para estimar los parámetros del modelo, este puede ser significativamente deficiente en datos que no han sido utilizados para calcularlo, como el **out-of-sample set** (datos futuros) en la práctica.

El segundo conjunto de validación representa esos datos, y el desempeño de este conjunto de validación es una mejor aproximación del desempeño que uno podría esperar del modelo seleccionado en la práctica. Esta es la razón porque dividimos los datos en un conjunto de estimación y dos conjuntos de dos conjuntos de validación, utilizando cierta técnica de selección aleatoria.  Los datos de estimación y el primer conjunto de datos de validación son utilizados en los pasos 2-5(con unas pocas iteraciones), mientras que el segundo conjunto de datos de validación sólo se utiliza una vez al final de todo antes de hacer la decisión de negocios final basada en el análisis. Esta división puede ser, por ejemplo, 80% datos de estimación, 10% validación y 10% evaluación, dependiendo del número de observaciones (por ejemplo, cuando hay muchos datos, usted puede preservar sólo unos pocos cientos de estos para los conjuntos de validación y evaluación, y el resto para la estimación).

Mientras se preparan los conjuntos de estimación y validación, usted puede también verificar que la proporción de las clases se mantenga estable, por ejemplo, que el porcentaje de personas que desean comprar un bote sea similar en ambos conjuntos, usted debe mantener el mismo balance de la variable dependiente cómo en los datos iniciales.

Por simplicidad, en estos apuntes **no** haremos iteraciones de los pasos 2-5. En la práctica, sin embargo, deberíamos iterar los pasos 2-5 unas cuantas veces utilizando el primer conjunto de validación cada vez, y hacer nuestra evaluación final del modelo utilizando el conjunto de evaluación sólo una vez.

```{r echo=FALSE}
if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectData),floor(estimation_data_percent*nrow(ProjectData)/100))
  non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids) #setdiff(x,y) returns the elements of x that are not in y
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectData)/100)
    non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectData), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectData[estimation_data_ids,]
validation_data=ProjectData[validation_data_ids,]
test_data=ProjectData[test_data_ids,]
```

A los tres conjuntos de datos se les denomina típicamente como **estimation_data** (conjunto de estimación `r estimation_data_percent`%), **validation_data** (conjunto de validación `r validation_data_percent`%) y **test_data** (conjunto de evaluación `r 100 - estimation_data_percent  -  validation_data_percent`%). 

En nuestro caso nosotros utilizamos`r nrow(estimation_data)` observaciones para el conjunto de estimación, `r nrow(validation_data)` en el conjunto de validación y `r nrow(test_data)` en el conjunto de evaluación. 


## Paso 2: Elegir la variable dependiente

Primero, asegurarse que la variable dependiente ha sido definida como categórica 0-1. En este ejemplo ilustrativo, usamos el default (o no default) como nuestra variable dependiente.


Los datos, sin embargo, pueden no estar disponibles inmediatamente como variables categóricas. Supongamos una cadena minorista quiere entender que discrimina los consumidores leales de los que no lo son. Si ellos tienen datos de las cantidades que los consumidores gastan en su tienda o la frecuencia de sus compras, ellos pueden crear una variable categórica ("leal vs no leal") utilizando una definición como: "Un consumidor leal es aquel que gasta X cantidad en la tienda y hace al menos Y compras al año". Ellos pueden codificar con "1" a los consumidores leales y con "0" a los que no lo son.  Ellos también podrían establecer determinados límites de "X" e "Y", y esta decisión puede tener un impacto significativo en el análisis global. Esta decisión puede ser la más crucial de todo el análisis de datos: una selección incorrecta en este paso podría llevar a un desempeño pobre más adelante, así como ideas irrelevantes. Uno debería revisar varias veces la elección realizada en este paso, iterando los pasos 2-3 y 2-5.


> Decidir cuidadosamente la definición de la variable dependiente 0/1 puede ser la parte más crítica del proceso de clasificación. Esta decisión típicamente depende del conocimiento contextual y debe revisarse varias veces a lo largo de un proyecto de analítica de datos.

En nuestros datos, el número 0/1 del conjunto de estimación se distribuye de esta forma:

```{r echo=FALSE}
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```

mientras que en el conjunto de validación son:

```{r echo=FALSE}
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```


## Paso 3: Análisis simple

La buena analítica de datos comienza con un buen conocimiento contextual, así como con análisis exploratorio simple y visualizaciones elementales. En el caso de la clasificación, uno puede explorar esas "clasificaciones simples"" evaluando como difieren las clases a lo largo de cualquiera de las variables independientes. Por ejemplo, estas son las estadísticas de nuestras variables independientes a lo largo de las dos clases, clase 1, "default":

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,independent_variables]),2))
```

y clase 0, "no default":

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,independent_variables]),2))
```

El propósito de dicho análisis de clases es obtener una idea inicial o una intuición de si las clases son en efecto separables, así como entender cuáles de las variables independientes tienen más poder discriminativo.

Notemos sin embargo que:

> Aunque cada variable independiente podría no diferir entre clases, la clasificación podrían aún ser posible: una combinación (lineal o no lineal) de variables independientes podría aún ser discriminativa.

Una simple herramienta de visualización que puede evaluar el poder discriminativo de las variables independientes es el **box plot** (diagrama de caja). Esa visualización representa simple resumen de las estadísticas de una variable independiente (media, mediana, máximo, mínimo, cuantiles, etc.) Por ejemplo, considere el box plot de nuestros datos para las variables de estatus de pago en la clase 1: 

```{r echo=FALSE, fig.height=4.5}
# Please ENTER the selected independent variables for which to draw box plots. 
# Please use numbers, not column names. E.g., c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8.
boxplots_independent_variables = c(7:12) # use only the PAY_ variables

DVvalues = unique(estimation_data[,dependent_variable])
x0 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[1]),boxplots_independent_variables]
x1 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[2]),boxplots_independent_variables]
colnames(x0) <- 1:ncol(x0)
colnames(x1) <- 1:ncol(x1)

swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x1))))
ggplot(melt(cbind.data.frame(n=1:nrow(x1), x1), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x1)))
set_swatch(swatch.default)
```

y la clase 0:

```{r echo=FALSE, fig.height=4.5}
swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x0))))
ggplot(melt(cbind.data.frame(n=1:nrow(x0), x0), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x0)))
set_swatch(swatch.default)
```

**Preguntas:**

1. Dibuje los "box plot"" de las clases 1 y clase 0 para otro conjunto de variables independientes de su elección.
2. ¿Cuáles variables independientes parecen tener el mayor poder discriminativo?

**Respuestas:**

*
*


## Paso 4: Clasificación e Interpretación
Una vez se decide cuáles variables dependientes e independientes a utilizar (las cuáles pueden ser revisadas en iteraciones posteriores), uno puede usar un gran número de métodos de clasificación para desarrollar un modelo que pueda separar ambas clases.

> Algunos de los métodos de clasificación utilizados ampliamente son: árboles de clasificación y regresión (CART), boosted trees, support vector machines, redes neuronales, k vecinos más cercanos (KNN), regresión logística, lasso, bosques aleatorios, deep learning methods, etc. 

En estos apuntes, por simplicidad solamente vamos a considerar **regresión logística** y **árboles de decisión (CART)**. Sin embargo, reemplazar estos modelos por los otros es relativamente simple (aunque algún conocimiento de cómo funcionan esos métodos siempre es necesario- vea la ayuda de R para los métodos si es necesario). Entender cómo funcionan esos métodos va más allá del alcance de estos apuntes, pero hay muchas fuentes de información disponibles en línea para todos los métodos de clasificación.

La **regresión logística** es un método similar a una regresión lineal con la excepción que la variable dependiente es discreta (por ejemplo 0 o 1). La regresión logística lineal estima los coeficientes de un modelo lineal utilizando las variables independientes seleccionadas mientras se optimiza un criterio de clasificación. Por ejemplo, estos son los parámetros de la regresión logística para nuestros datos: 

```{r echo=FALSE}
# We first turn the data into data.frame's
estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~") # When drop is FALSE, the dimensions of the object are kept. head(x,-1) returns all but the last element of x.

logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)
log_coefficients <- round(summary(logreg_solution)$coefficients,1)

knitr::kable(round(log_coefficients,2))
```

Dado un conjunto de variables independientes, el resultado de la regresión logística esperada (la suma de productos de las variables independientes con los correspondientes coeficientes de la regresión) puede ser utilizada para evaluar la probabilidad de que una observación pertenezca a determinada clase. Específicamente, el resultado de la regresión puede ser transformado en una probabilidad de pertenecer a, digamos, la clase 1 de cada observación. La probabilidad estimada de que una observación pertenece a la clase 1 en nuestro conjunto de validación (por ejemplo, probabilidad de default) para las primeras observaciones utilizando la regresión logística es:

```{r echo=FALSE}
# Let's get the probabilities for the 3 types of data from the logistic regression
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

# Let's get the decision of the logistic regression for the 3 types of data 
estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_log,validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
```

La decisión tradicional es clasificar cada observación en el grupo con la mayor probabilidad, pero uno puede cambiar esa decisión, como veremos más adelante.

Seleccionar el mejor subconjunto de variables independientes para la regresión logística, un caso especial del problema general de **selección de variables**, es un proceso iterativo donde se toman en cuenta tanto la significancia de los coeficientes de la regresión como el desempeño del modelo en el primer conjunto de validación. Unas cuantas variantes son evaluadas en la práctica, cada una terminando en diferentes resultados.


**CART** es un método ampliamente utilizado porque el modelo de clasificación estimado es fácil de interpretar. Esta herramienta de clasificación divide iterativamente los datos usando la variable independiente más discriminativa en cada paso, construyendo un árbol (cómo se muestra más adelante) en el camino. Los métodos CART **limitan el tamaño del árbol** utilizando varias técnicas estadísticas para evitar el **sobreajuste**. Por ejemplo, utilizando las funciones rpart y rpart.control de R, podemos limitar el tamaño del árbol seleccionando las funciones **complexity control** parámetro **cp** (lo que esto hace va más allá del alcance de estos apuntes. Para las funciones rpart y rpart.control en R, pequeños valores, por ejemplo cp=0.001, conducen a árboles más grandes, como veremos más adelante).


> Uno de los riesgos más grande cuando se construye un modelo de clasificación es el sobreajuste (overfitting): mientras siempre sea trivial construir un modelo (por ejemplo un árbol) que clasifique cualquier conjunto (estimación)  de datos de manera perfecta sin ningún error, pero no se garantiza que la calidad del clasificador en el conjunto de validación sea cercana a la del conjunto de validación. Encontrar el balance correcto entre "sobreajuste" y "desajuste" (**overfitting** y **underfitting**) es uno de los aspectos más importantes de la analítica de datos. Aunque existen algunas técnicas estadísticas que nos ayudan a encontrar ese balance (incluyendo el conjunto de validación) es más una combinación de buen análisis estadístico y buen criterio cualitativo (con respecto a la simplicidad o interpretabilidad de los modelos estimados) la que nos llevará a construir modelos de clasificación que funcionen bien en la práctica.

Al implementar un modelo CART con complexity control cp=`r CART_cp`,  nos lleva al siguiente árbol(**NOTA**: para mejor visibilidad del árbol,  llamaremos a las variables independientes IV1 a `r paste("IV", length(independent_variables), sep="")` cuando se use un CART):

```{r echo=FALSE}
# Name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

Las hojas del árbol indican el número de observaciones del conjunto de estimación que pertenecen a cada clase y alcanzan "esa hoja". Una clasificación perfecta sólo tendría un dato de cada clase en cada hoja. Sin embargo, una clasificación tan perfecta es más propensa o no clasificar bien los datos de validación debido al sobreajuste en los datos de estimación.

```{r echo=FALSE}
# Tree parameter
# Please ENTER the new tree (CART) complexity control cp 
CART_cp = 0.00068
```

Uno puede estimar árboles más grandes cambiando el parámetro **complexity control** del modelo (en este caso rpart.control argument cp). Por ejemplo, así es como se vería un árbol con cp=`r toString(CART_cp)`:

```{r echo=FALSE}
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = CART_cp))
rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

Uno puede también usar el porcentaje de datos en cada hoja del árbol para tener una probabilidad estimada de las observaciones que pertenecen a esa clase. La **pureza de la hoja** puede indicar la probabilidad de que una observación que alcanza una hoja pertenezca a determinada clase. En nuestro caso, la probabilidad de nuestros datos de validación de pertenecer a la clase 1(por ejemplo, que un cliente hará default) para las primeras observaciones, usando el CART de arriba es:

```{r echo=FALSE}
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]

estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_tree,validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_prediction_class_tree_large,validation_Probability_class1_tree_large)
rownames(Classification_Table_large)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
```

La tabla de arriba asume un **umbral de probabilidad** para considerar una observación como parte de la "clase 1" de `r Probability_Threshold`. En la práctica necesitamos seleccionar un umbral de probabilidad: esta es una decisión muy importante que discutiremos más adelante.

**Pregunta:**

Calcule el CART con un parámetro de complejidad de cp=0.0001 o menor. ¿Es práctico correrlo? ¿Es práctico interpretarlo? ¿Confía en el clasificador?

**Respuesta:**

*


Ya hemos discutido la selección de variables y control de complejidad para los métodos de clasificación. En nuestro caso podemos ver la importancia relativa de las variables independientes la `variable.importance` de los árboles de decisión (ver `help(rpart.object)` en R) o los valores de Z para la regresión logística. Para visualizarlo más fácil, se escalaron los valores entre -1 y 1 (la escala se hace para cada método de forma separada, note que CART no entrega el signo de los coeficientes). De esta tabla podemos ver los **factores clave** de la clasificación de acuerdo a cada modelo.

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))

tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)

large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)

Importance_table <- cbind(log_importance,tree_importance_final,large_tree_importance_final)
colnames(Importance_table) <- c("Logistic Regression", "CART 1", "CART 2")
rownames(Importance_table) <- rownames(log_importance)
## printing the result in a clean-slate table
knitr::kable(round(Importance_table,2))
```

En general, no es necesario que todos los modelos coincidan en los factores más importantes: cuando hay una diferencia mayor, particularmente entre modelos que tengan resultados satisfactorios como discutiremos después, podríamos necesitar cambiar el análisis global, incluyendo el objetivo del análisis, así como los datos usados, ya que los resultados podrían no ser robustos. **Como siempre, interpretar y usar los resultados de analítica de datos requiere un balance entre análisis cuantitativo y cualitativo.**



## Paso 5: Precisión del conjunto de validación.

Utilizando las probabilidades de las predicciones del conjunto de validación, cómo se mencionó antes, se pueden generar medidas de desempeño de la clasificación. Antes de discutirlas, notemos que, dada la probabilidad de pertenecer a una clase, **una opción de predicción razonable sería simplemente predecir la clase que tenga la probabilidad de ocurrencia más alta (o la más frecuente)**. Sin embargo, esta no tiene que ser la única opción en la práctica.

> Seleccionar un umbral de probabilidad para decidir la clase a la que pertenece una observación es una decisión importante que el usuario debe hacer. Mientras en algunos casos una probabilidad razonable es 50%, en otros debería ser 99.9% o 0.01%. ¿pueden pensar acerca de esos casos?

**Preguntas:**

1. Puede pensar sobre tal escenario:
2. ¿Qué límite de probabilidad usted piensa que haría más sentido en el caso de default en tarjetas de crédito?

**Respuestas:**

*
*

Para diferentes opciones de umbral de probabilidad, uno puede medir varias métricas de desempeño, las cuales se describen a continuación.

### 1.  Proporción de aciertos (Hit ratio)
Es simplemente el porcentaje de observaciones que han sido correctamente clasificadas (la clase predecida es la clase real). Podemos simplemente contar el número de datos de validación clasificados correctamente y dividir ese número entre el número total de observaciones del conjunto de validación, utilizando dos CART y una regresión logística. Los siguientes son los porcentajes de acierto utilizando un umbral de probabilidad de `r Probability_Threshold*100`%:

```{r echo=FALSE}
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_log,
                               validation_prediction_class_tree,
                               validation_prediction_class_tree_large)
validation_hit_rates = rbind(
  100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual),
  100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual)
  )
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("Logistic Regression", "First CART", "Second CART")
knitr::kable(validation_hit_rates)
```
Mientras que para los datos de estimación, las tasas de precisión son:
```{r echo=FALSE}
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_log,
                               estimation_prediction_class_tree,
                               estimation_prediction_class_tree_large)
estimation_hit_rates = rbind(
  100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual),
  100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual)
  )
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(estimation_hit_rates)
```

Un simple punto de referencia para comparar el desempeño de un modelo de clasificación es el **criterio de la máxima oportunidad**. Este mide la proporción de la clase más grande. Para nuestros datos de validación el grupo más grande son las personas que no entran en default: `r sum(!validation_actual)` de `r length(validation_actual)` personas. Claramente, si clasificamos todos los individuos dentro del grupo más grande, podemos tener una tasa de aciertos del `r round(100*sum(!validation_actual)/length(validation_actual), 2)`%, sin hacer ningún trabajo. Uno puede alcanzar al menos una tasa de aciertos de al menos la máxima oportunidad, aunque como veremos más adelante aún hay más criterios de desempeño a considerar.


### 2. Matriz de Confusión.

La matriz de confusión muestra el número de datos que ha sido clasificado correctamente para cada clase. Por ejemplo, para el método con la tasa de aciertos más alta en los datos de validación (entre la regresión logística y los dos modelos CART), y para un umbral de probabilidad del `r Probability_Threshold*100`%, la matriz de confusión es:

```{r echo=FALSE}
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(validation_prediction_best*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!validation_prediction_best)*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```

**Preguntas:**

1. Note que los porcentajes suman 100% para cada fila. ¿Por qué?
2. Además, una "buena" matriz de confusión debe tener grandes valores en su diagonal y pequeños valores en las otras posiciones. ¿Por qué?

**Respuestas:**

*
*



### 3. Curva ROC.

Recuerde que cada observación es clasificada por nuestro modelo de acuerdo a las probabilidades Pr(0) y Pr(1) y un determinado umbral de probabilidad. Comúnmente se establece el umbral de probabilidad en 0.5 (así que las observaciones con Pr(1)>0.5 son clasificadas como 1). Sin embargo, podemos variar este umbral, por ejemplo si estamos interesados en clasificar correctamente todos los 1's pero no nos interesa perder los 0's (y viceversa).

Cuando cambiamos el umbral de probabilidad obtenemos diferentes valores de proporción de aciertos, falsos positivos, falsos negativos, o cualquier otra medida de desempeño. Podemos graficar, por ejemplo, cómo cambian los falsos positivos vs los verdaderos positivos a medida cambiamos el umbral de probabilidad, y generamos la curva ROC.

Las curvas ROC para los datos de validación para ambos CARTs arriba así como para la regresión logística son:

```{r echo=FALSE}
validation_actual_class <- as.numeric(validation_data[,dependent_variable])

pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)

test1<-performance(pred_tree, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive rate CART 1")
plot1 <- ggplot(df1, aes(x=`False Positive rate CART 1`, y=`True Positive rate CART 1`)) + geom_line()
test2<-performance(pred_log, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate log reg", "True Positive rate log reg")
plot2 <- ggplot(df2, aes(x=`False Positive rate log reg`, y=`True Positive rate log reg`)) + geom_line()
test3<-performance(pred_tree_large, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate CART 2", "True Positive rate CART 2")
plot3 <- ggplot(df3, aes(x=`False Positive rate CART 2`, y=`True Positive rate CART 2`)) + geom_line()

# We can plot the curves individually 
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive rate ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

¿Cómo debería verse una buena curva ROC? La regla básica para evaluar una curva ROC es que entre más "alta" mejor, porque el área bajo la curva es más grande. Usted podría también seleccionar un punto en la curva ROC (el mejor para nuestro propósito) y utilizar que el desempeño de los falsos positivos y falsos negativos (y su correspondiente umbral para P(1)) para evaluar el modelo.

**Preguntas:**

1. ¿Que punto en la curva ROC seleccionaría?
2. ¿A qué clasificador corresponde la línea punteada de 45°? ¿Cómo muestra la curva ROC que tanto la regresión logística como modelos CART son superiores a dicho clasificador?


**Respuestas:**

*
*

### 4. Curva Gains
La curva Gains es una técnica popular en ciertas aplicaciones, cómo en marketing o riesgo de crédito. 

Como un ejemplo concreto, considere el caso de una campaña de marketing por correo. Digamos que tenemos un clasificadorque intenta identificar las personas que si responderán a la encuesta. Nosotros quisieramos seleccionar los menos casos posibles y aún así capturar el máximo número de encuestas posibles.

Podemos medir el porcentaje de todas las respuestas que el clasificador captura, digamos, x% de los casos: el mejor x% en términos de probabilidad de respuesta asignada por nuestro clasificador. Para cada porcentaje de los casos que seleccionamos (x), podemos graficar el siguiente punto: el eje-x será el porcentaje de todos los casos que fueron seleccionados, mientras que el eje-y será el porcentaje de todos los casos pertenecientes a la clase 1 de los que fueron seleccionados (la proporción de verdaderos positivos/ predecidos positivos de nuestro clasificador, asumiendo que el clasificador predice clase 1 para todos los casos, y predice clase 0 para los restantes). Si graficamos esos puntos mientras cambiamos el porcentaje de cases seleccionados (x) (mientras se cambia el umbral de probabilidad del clasificador), obtenemos una gráfica llamada la **curva gains**. 

En el caso de default de tarjetas de crédito, la curva gains para los datos de validación de nuestros tres clasificadores son las siguientes:

```{r echo=FALSE}
validation_actual <- validation_data[,dependent_variable]
all1s <- sum(validation_actual); 

probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % of validation data`, y=`CART 1 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % of validation data`, y=`CART 2 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % of validation data`, y=`log reg % of class 1`)) + geom_line()

# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of validation data selected"
  df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Note que si fuéramos a examinar casos aleatorios, en lugar de seleccionar los "mejores" usando un clasificador informado, la "predicción aleatoria" de la curva gains sería una línea recta de 45 grados.

**Pregunta:**

¿Por qué?

**Respuesta:**

*
¿Cómo debería verse una buena curva gains? Entre más arriba de la línea de 45° esté nuestra curva gains, mejor. Además, así como la curva ROC, uno puede seleccionar el porcentaje de los casos de todos los casos a examinar apropiadamente de modo que en cualquier punto de la curva gains sea seleccionado.

**Pregunta:**

¿Cuál punto de la curva gains deberíamos seleccionar en la práctica?

**Respuesta:**

*

### 5. Curva de Ganancias

Finalmente, podemos generar la llamada curva de ganancias, la cual se utiliza frecuentemente para tomar la decisión final. Considere una campaña directa de marketing, y suponga el $costo de enviar publicidad como 1, y una ganancia esperada de una persona que responde positivamente de $45. Suponga que tiene una base de datos de 1 millón de personas a las que se les puede potencialmente enviar publicidad.  Las típicas tasas de respuesta son 0.05%. ¿Qué fracción del millón de personas debería enviarle la publicidad? 

Para responder este tipo de preguntas necesitamos crear la **curva de ganancias**. Podemos medir cierta métrica de ganancias si solo seleccionamos los mejores casos en términos de probabilidad de respuesta asignada por nuestro clasificador. Luego podemos graficar la curva de ganancias mientras cambiamos el porcentaje de casos seleccionados así como hicimos con la curva gains, y calculando la correspondiente **ganancia estimada** (o pérdida). Esto es simplemente igual a:

> Ganancia total esperada = (% de 1's correctamente predecidos) x (valor de capturar un 1) + (% de 0's correctamente predecidos) x (valor de capturar un 0) + (% de 1's incorrectamente predecidos como 0) x (valor de perder un 1) + (% de 0's incorrectamente predecidos como 1) x (valor de perder un 0)

> Calcular la ganancia esperada requiere de tener una estimación de los cuatro valores: valor de capturar un 0 o 1, y costo de perder un 0 o 1 o viceversa.

Dados los valores y costos de clasificar correcta e incorrectamente, podemos graficar la ganancia total esperada (o pérdida) a medida cambiamos el porcentaje de casos seleccionados, por ejemplo cuando usamos el umbral de probabilidad como cuando generamos las curvas ROC. 

En el caso de riesgo de crpedito, nosotros consideramos las siguientes pérdidas y ganancias por clasificar a los clientes:

```{r echo=FALSE}
knitr::kable(Profit_Matrix)
```

Basado en esos estimados de ganancias y pérdidas, las curvas de ganancias para los datos de validación de los tres clasificadores son:

```{r echo=FALSE}
actual_class <- validation_data[,dependent_variable]

probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame1 <- data.frame(
  `CART 1 % selected` = res[1,],
  `CART 1 est. profit` = res[2,],
  check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % selected`, y=`CART 1 est. profit`)) + geom_line()

probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame2 <- data.frame(
  `CART 2 % selected` = res[1,],
  `CART 2 est. profit` = res[2,],
  check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % selected`, y=`CART 2 est. profit`)) + geom_line()

probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame3 <- data.frame(
  `log reg % selected` = res[1,],
  `log reg est. profit` = res[2,],
  check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % selected`, y=`log reg est. profit`)) + geom_line()

# we can plot the curves individually 
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot

# But we're going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" est. profit", "", df$variable)
  colnames(df)[1] <- "% of validation data selected"
  df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
```

Podemos ahora seleccionar el porcentaje de casos seleccionados que corresponde la máxima ganancia estimada (o pérdida mínima, si es necesario).


**Pregunta:**

¿Qué punto de la curva de ganancia debería seleccionar en la práctica?

**Respuesta:**

*

Notemos que para maximizar la ganancia esperada necesitamos tener una estimación de la ganancia o pérdida para cada uno de los cuatro escenarios! Esto puede ser difícil de evaluar, por lo que normalmente se necesita realizar cierto análisis de sensibilidad a las ganancias y pérdidas que han sido asumidas. Por ejemplo, podemos generar diferentes curvas de ganancia (el peor caso, el mejor caso, y el caso promedio) y ver cuánto varía la mejor ganancia, y más importante ver **como varía nuestro modelo de clasificación y umbral de probabilidad correspondiente a la mejor ganancia**, ya que es lo que eventualmente debemos decidir.


## Paso 6: Precisión del modelo

Habiendo iterado los pasos 2-5 hasta que estemos satisfechos con el desempeño del modelo en los datos de validación, en este paso necesitamos hacer el análisis del desempeño hecho en el paso 5 para los datos de evaluación (test set). Esta es la medida de desempeño que mejor representa lo que uno esperaría en la práctica una vez implementada la solución, **asumiendo (como siempre) que los datos utilizados en el análisis son representativos de la situación en la que será implementado el sistema.**

Veamos en nuestro caso la **hit ratio, matriz de confusión, curva ROC, curva gains, y curva de ganancias** para los datos de evaluación. Para el the hit ratio y la matriz de confusión usamos `r Probability_Threshold*100`% como el umbral de probabilidad de la clasificación.

```{r echo=FALSE}
######for train data#####
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_log,
                         test_prediction_class_tree,
                         test_prediction_class_tree_large
                         )
test_hit_rates = rbind(
  100*sum(test_prediction_class_log==test_actual)/length(test_actual),
  100*sum(test_prediction_class_tree==test_actual)/length(test_actual), 
  100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual)
  )
colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- c("Logistic Regression","First CART", "Second CART")

knitr::kable(test_hit_rates)
```

La matriz de confusión para el modelo con la mejor proporción de aciertos en nuestros datos de validación:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
test_prediction_best = test_predictions[which.max(test_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```

Curva ROC para los datos de evaluación:

```{r echo=FALSE}
test_actual_class <- as.numeric(test_data[,dependent_variable])

pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)

test<-performance(pred_tree_test, "tpr", "fpr")
df1<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive CART 1")
test2<-performance(pred_tree_large_test, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART 2", "True Positive CART 2")
test3<-performance(pred_log_test, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate log reg", "True Positive log reg")

df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Curva Gains para los datos de evaluación:

```{r echo=FALSE}
test_actual <- test_data[,dependent_variable]
all1s <- sum(test_actual)

probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)

df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of test data selected"
  df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Finalmente, curva de ganancias para los datos de evaluación, usando las mismas estimaciones de pérdidas y ganancias que hicimos arriba:

```{r echo=FALSE}
actual_class<- test_data[,dependent_variable]

probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame1 <- data.frame(
  `CART 1 % selected` = res[1,],
  `CART 1 est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame2 <- data.frame(
  `CART 2 % selected` = res[1,],
  `CART 2 est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame3 <- data.frame(
  `log reg % selected` = res[1,],
  `log reg est. profit` = res[2,],
  check.names = FALSE
)

df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" est. profit", "", df$variable)
  colnames(df)[1] <- "% of test data selected"
  df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
```

**Preguntas:**

1. ¿Es el desempeño en los datos de evaluación similar al desempeño en los datos de validación? ¿Deberíamos esperar que el desempeño de nuestra clasificación sea cercano al que se obtienen en los datos de evaluación cuando se implementa en producción? ¿Porqué o porqué no? ¿Que deberíamos hacer si son diferentes?
2. Haga una evaluación final acerca de cuál clasificador debería ser usado (de los considerados aquí) para la clasificación de default en tarjetas de crédito, con qué porcentaje de casos/umbral de probabilidad, y diga la razón. ¿Cuál es la ganancia que la compañía podría lograr (como se midió con los datos de evaluación) basado en su solución?
3. ¿Qué tanto depende su evaluación de los valores de correcta o incorrectamente clasificar (`r actual_1_predict_1`, `r actual_1_predict_0`, `r actual_0_predict_1`, `r actual_0_predict_0`)?
4. ¿Cuál decisión de negocios puede hacer el emisor de tarjetas de crédito basado en este análisis?


**Respuestas:**

*
*
*
*