---
title: "Clasificación en Pago de Tarjetas de Crédito - Más Métodos de Clasificación"
author: "Theos Evgeniou, Spyros Zoumpoulis. Traducción por Eduardo Aguilar, UCA."
output:
  html_document:
    css: ../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../AnalyticsStyles/default.sty
always_allow_html: yes
---

<!-- **Nota:** Asumiendo que el directorio de trabajo es  "MYDIRECTORY/INSEADAnalytics" (donde usted ha clonado el material del curso), puede crear un archivo html corriendo en la consola el comando rmarkdown::render("CourseSessions/ClassificationProcessCreditCardDefault.Rmd") -->

```{r echo=FALSE, message=FALSE}
make_pdf_file = 0 # Haga este número 1 si quiere un archivo PDF, 0 para HTML

source("../../AnalyticsLibraries/library.R")
source("../../AnalyticsLibraries/heatmapOutput.R")

# Opciones de paquetes
ggthemr('fresh')  # tema de ggplot
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.2)
options(knitr.kable.NA = '')
```

```{r echo=FALSE, message=FALSE}
# Por favor INGRESE el nombre del archivo
datafile_name = "../Data/UCI_Credit_Card.csv"
ProjectData <- read.csv(datafile_name)
# Convertimos los datos a la clase data.matrix para que sea mas fácil de manipular
ProjectData <- data.matrix(ProjectData)

# Por favor INGRESE la variable dependiente(clase).
# Por favor use números, no nombres de columnas. Por ejemplo, 82 usa la columna en la posición 82 como la variable dependiente.
# Necesita asegurarse que la variable dependiente solo tome dos valores: 0 y 1.
dependent_variable = 25

# Por favor ingrese los atributos a usar como variables independientes.
# Por favor use números, no nombres de columnas. Por ejemplo, c(1:5, 7, 8) usa las columnas 1,2,3,4,5,7,8.
independent_variables = c(1:24) # usa todos los atributos disponibles

# Ingeniería de variables
if (1){
  tmpx = t(apply(ProjectData[,7:12], 1, function(r) matrix(c(sum(r==-2), sum(r==-1), sum(r==0),sum(r > 0)), nrow=1)))
  colnames(tmpx) <- c("NoConsum","PayDuly", "Delay1Month","DelayMoreMonths")
  ProjectData_tmp = cbind(ProjectData[,2:5],tmpx, apply(ProjectData[,13:18], 1, function(r) median(r[!is.na(r)])), apply(ProjectData[,19:24]/ProjectData[,13:18], 1, function(r) ifelse(sum(!is.na(r) & !is.infinite(r)), mean(r[!is.na(r) & !is.infinite(r)]),0)),ProjectData[,25])
  colnames(ProjectData_tmp) <- c(colnames(ProjectData)[2:5], colnames(tmpx),"MedianBill", "MedianPercentPay", colnames(ProjectData)[25])
  ProjectData = ProjectData_tmp
  dependent_variable = 11
  independent_variables = c(1:10) # usa todos los atributos disponibles
}

dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectData), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectData), max(i,1))))

if (length(unique(ProjectData[,dependent_variable])) !=2){
  cat("\n*****\n BE CAREFUL, THE DEPENDENT VARIABLE TAKES MORE THAN 2 VALUES")
  cat("\nSplitting it around its median...\n*****\n ")
  new_dependent = ProjectData[,dependent_variable] >= median(ProjectData[,dependent_variable])
  ProjectData[,dependent_variable] <- 1*new_dependent
}

# Por favor INDIQUE qué método de clasificación quisiera correr para examinar la precisión. 1 para usar un método, 0 para no usarla. Por favor sólamente cambie los 0/1's, nada más.
# Regresión Logística
logreg = 1
# Regresión Logística Regularizada
reg_logreg = 1
# CART 
cart = 1
cart_large = cart
# XGBoost
XGBoost = 1

methods = c(logreg, cart, cart_large, reg_logreg, XGBoost)
names_of_methods <- c("Logistic Regression", "First CART", "Second CART", "Regularized Logistic Regression", "XGBoost")

methods_short = methods[-3]
names_of_methods_short <- names_of_methods[-3]
names_of_methods_short[2] <- "CART"

# Por favor INGRESE el umbral de probabilidad arriba del cual una observación será categorizada como clase 1:
Probability_Threshold = 0.5 # entre 0 y 1

# Por favor INGRESE el porcentaje de datos usados para la estimación
estimation_data_percent = 80
validation_data_percent = 10
test_data_percent = 100-estimation_data_percent-validation_data_percent

# Por favor INGRESE 1 si quiere dividir los datos aleatoriamente en los conjuntos de estimación y validación/evaluación
random_sampling = 0

# Parámetros del árbol
# Por favor INGRESE el control de complejidad cp del árbol(CART), por ejemplo de 0.0001 a 0.02, dependiendo de los datos.
CART_cp = 0.0025
CART_control = rpart.control(cp = CART_cp)

# Por favor INGRESE el significado de las clases 1 y 0:
class_1_interpretation = "default"
class_0_interpretation = "no default"

# Por favor INGRESE los valores de ganancia/costo de clasificar correcta o incorrectamente los datos:
actual_1_predict_1 = 0
actual_1_predict_0 = -100000
actual_0_predict_1 = 0
actual_0_predict_0 = 20000

Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix) <- c(paste("Predict 1 (", class_1_interpretation, ")", sep = ""), paste("Predict 0 (", class_0_interpretation, ")", sep = ""))
rownames(Profit_Matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))

# Por favor INGRESE el máximo número de observaciones a mostrar en el reporte y diapositivas 
# (El número predefinido es 50. Si el número es demasiado grande el reporte y las diapositivas podrían no ser generados -  muy lento o no funcionará!!)
max_data_report = 10 
```

# El Contexto de Negocios
Un emisor de tarjetas de crédito de Taiwan quiere mejorar la predicción de la probabilidad de impago de sus clientes, así como identificar las carcaterísticas clave que determinan esta probabilidad. Esto informaría al emisor acerca de a quien dar una tarjeta así como que límite dar. También ayudaría  al emisor a tener un mejor entendimiento de sus actuales y potenciales consumidores, lo cuál afectaría su futura estrategia, incluyendo sus planes de ofrecer productos crediticios específicos a sus consumidores.


<hr>\clearpage

# Los Datos
(Fuente: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset . Reconocemos lo siguiente:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.)

El emisor de tarjetas de crédito ha reunido información de `r nrow(ProjectData)` consumidores. Los datos contienen información de `r length(independent_variables)` variables, incluyendo factores demográficos, datos de crédito, historial de pagos, y estados de cuenta de tarjetas de crédito desde Abril 2005 a Septiembre 2005, así como información del resultado: El cliente pagó o no?

Nombre                       | Descripción
:--------------------------|:--------------------------------------------------------------------
ID                         | ID de cada cliente
LIMIT_BAL                  | Monto del crédito en dólares (incluyendo créditos individuales y familiares/suplementarios)
SEX                        | Género (1=masculino, 2=femenino)
EDUCATION                  | (1=postgrado, 2=universidad, 3=bachillerato, 4=otros, 5=desconocido, 6=desconocido)
MARRIAGE                   | Estatus Marital (1=casado, 2=soltero, 3=otros)
AGE                        | Edad en años
PAY_0                      | Estatus de pago en Septiembre 2005 (-2=no consumo, -1=paga debidamente, 0=usa crédito rotativo, 1=atraso de un mes, 2=atraso de dos meses, ... 8=atraso de ocho meses, 9=atraso de nueve meses y más)
PAY_2                      | Estatus de pago en Agosto 2005 (misma escala)
PAY_3                      | Estatus de pago en Julio 2005 (misma escala)
PAY_4                      | Estatus de pago en Junio 2005 (misma escala)
PAY_5                      | Estatus de pago en Mayo 2005 (misma escala)
PAY_6                      | Estatus de pago en Abril 2005 (misma escala)
BILL_AMT1                  | Monto a pagar en Septiembre 2005 (dólares)
BILL_AMT2                  | Monto a pagar en Agosto 2005 (dólares)
BILL_AMT3                  | Monto a pagar en Julio 2005 (dólares)
BILL_AMT4                  | Monto a pagar en Junio 2005 (dólares)
BILL_AMT5                  | Monto a pagar en Mayo 2005 (dólares)
BILL_AMT6                  | Monto a pagar en Abril 2005 (dólares)
PAY_AMT1                   | Monto de pago anterior en Septiembre 2005 ( dólares)
PAY_AMT2                   | Monto de pago anterior en Agosto 2005 ( dólares)
PAY_AMT3                   | Monto de pago anterior en Julio 2005 ( dólares)
PAY_AMT4                   | Monto de pago anterior en Junio 2005 ( dólares)
PAY_AMT5                   | Monto de pago anterior en Mayo 2005 ( dólares)
PAY_AMT6                   | Monto de pago anterior en Abril 2005 ( dólares)
default.payment.next.month | Default (1=si, 0=no)

Veamos la información de unos pocos consumidores. Así es como las primeras `r min(max_data_report, nrow(ProjectData))` filas de un total de `r nrow(ProjectData)` se ven (transpuestas, por conveniencia):

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
knitr::kable({
  df <- t(head(round(ProjectData[,independent_variables],2), max_data_report))
  colnames(df) <- sprintf("%02d", 1:ncol(df))
  df
})
```

<hr>\clearpage


# Un proceso de Clasificación

> Es importante recordar que los proyectos de analítica requieren de un delicado balance entre experimentación e intuición, pero también siguiendo un proceso. El valor de seguir un proceso es para evitar ser engañado por la aleatoriedad en los datos y encontrar "patrones y resultados" que pueden estar explicados principalmente por nuestro propio sesgo y no por hechos/datos en sí. 
**No existe tal cosa cómo el mejor proceso** de clasificación. Sin embargo, tenemos que empezar por algo, así que haremos uso del siguiente proceso:

1. Crear un conjunto de estimación y dos conjuntos de evaluación dividiendo los datos en tres grupos. Los pasos 2-5 a continuación serán efectuados solo en el conjunto de estimación y el primer conjunto de validación. Usted debe efectuar el paso 6 una vez en el segundo conjunto de validación, llamado **datos de evaluación**, y reportar el desempeño en ese (segundo) conjunto sólo para tomar la decisión de negocios final.
2. Establecer la variable dependiente (como una variable categórica 0-1; clasificación múltiple también es posible, y similar, pero no exploraremos en eso en estos apuntes).
3. Hacer un diagnóstico preliminar acerca de la importancia relativa de las variables explicativas utilizando herramientas de visualización y simples estadísticas descriptivas.
4. Estimar el modelo de clasificación utilizando los datos de estimación, e interpretando los resultados.
5. Evaluar la precisión de la clasificación en el primer conjunto de validación, posiblemente repitiendo os pasos 2-5 unas cuantas veces en diferentes maneras para mejorar el desempeño.
6. Finalmente, evaluar la precisión de la clasificación en el segundo modelo de validación. Usted eventualmente usará/reportará todas las medidas y gráficas de desempeño solamente en el segundo conjunto de validación. 

Sigamos esos pasos.

## Paso 1: Dividir los datos
Es muy importante que usted mida y reporte (o espera ver de parte del científico de datos encargado del proyecto) el desempeño de su modelo en **datos que no han sido utilizados durante el análisis, estos son llamados "out-of-sample set", "test dataset" o "conjunto de evaluación"** (pasos 2-5). La idea es que en la práctica deseamos utilizar los modelos para predecir la clase de observaciones que no han sido observadas aún (por ejemplo "los datos futuros"): aunque el desempeño del método de clasificación puede ser alto en los datos utilizados para estimar los parámetros del modelo, este puede ser significativamente deficiente en datos que no han sido utilizados para calcularlo, como el **out-of-sample set** (datos futuros) en la práctica.

El segundo conjunto de validación representa esos datos, y el desempeño de este conjunto de validación es una mejor aproximación del desempeño que uno podría esperar del modelo seleccionado en la práctica. Esta es la razón porque dividimos los datos en un conjunto de estimación y dos conjuntos de dos conjuntos de validación, utilizando cierta técnica de selección aleatoria.  Los datos de estimación y el primer conjunto de datos de validación son utilizados en los pasos 2-5(con unas pocas iteraciones), mientras que el segundo conjunto de datos de validación sólo se utiliza una vez al final de todo antes de hacer la decisión de negocios final basada en el análisis. Esta división puede ser, por ejemplo, 80% datos de estimación, 10% validación y 10% evaluación, dependiendo del número de observaciones (por ejemplo, cuando hay muchos datos, usted puede preservar sólo unos pocos cientos de estos para los conjuntos de validación y evaluación, y el resto para la estimación).

Mientras se preparan los conjuntos de estimación y validación, usted puede también verificar que la proporción de las clases se mantenga estable, por ejemplo, que el porcentaje de personas que desean comprar un bote sea similar en ambos conjuntos, usted debe mantener el mismo balance de la variable dependiente cómo en los datos iniciales.

Por simplicidad, en estos apuntes **no** haremos iteraciones de los pasos 2-5. En la práctica, sin embargo, deberíamos iterar los pasos 2-5 unas cuantas veces utilizando el primer conjunto de validación cada vez, y hacer nuestra evaluación final del modelo utilizando el conjunto de evaluación sólo una vez.

```{r echo=FALSE}
if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectData),floor(estimation_data_percent*nrow(ProjectData)/100))
  non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids) #setdiff(x,y) returns the elements of x that are not in y
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectData)/100)
    non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectData), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectData[estimation_data_ids,]
validation_data=ProjectData[validation_data_ids,]
test_data=ProjectData[test_data_ids,]
```

A los tres conjuntos de datos se les denomina típicamente como **estimation_data** (conjunto de estimación `r estimation_data_percent`%), **validation_data** (conjunto de validación `r validation_data_percent`%) y **test_data** (conjunto de evaluación `r 100 - estimation_data_percent  -  validation_data_percent`%). 

En nuestro caso nosotros utilizamos`r nrow(estimation_data)` observaciones para el conjunto de estimación, `r nrow(validation_data)` en el conjunto de validación y `r nrow(test_data)` en el conjunto de evaluación. 

## Paso 2: Elegir la variable dependiente
Primero, asegurarse que la variable dependiente ha sido definida como categórica 0-1. En este ejemplo ilustrativo, usamos el default (o no default) como nuestra variable dependiente.

Los datos, sin embargo, pueden no estar disponibles inmediatamente como variables categóricas. Supongamos una cadena minorista quiere entender que discrimina los consumidores leales de los que no lo son. Si ellos tienen datos de las cantidades que los consumidores gastan en su tienda o la frecuencia de sus compras, ellos pueden crear una variable categórica ("leal vs no leal") utilizando una definición como: "Un consumidor leal es aquel que gasta X cantidad en la tienda y hace al menos Y compras al año". Ellos pueden codificar con "1" a los consumidores leales y con "0" a los que no lo son.  Ellos también podrían establecer determinados límites de "X" e "Y", y esta decisión puede tener un impacto significativo en el análisis global. Esta decisión puede ser la más crucial de todo el análisis de datos: una selección incorrecta en este paso podría llevar a un desempeño pobre más adelante, así como ideas irrelevantes. Uno debería revisar varias veces la elección realizada en este paso, iterando los pasos 2-3 y 2-5.

> Decidir cuidadosamente la definición de la variable dependiente 0/1 puede ser la parte más crítica del proceso de clasificación. Esta decisión típicamente depende del conocimiento contextual y debe revisarse varias veces a lo largo de un proyecto de analítica de datos.

En nuestros datos, el número 0/1 del conjunto de estimación se distribuye de esta forma:
```{r echo=FALSE}
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```

mientras que en el conjunto de validación son:

```{r echo=FALSE}
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
```

## Paso 3: Análisis simple
La buena analítica de datos comienza con un buen conocimiento contextual, así como con análisis exploratorio simple y visualizaciones elementales. En el caso de la clasificación, uno puede explorar esas "clasificaciones simples"" evaluando como difieren las clases a lo largo de cualquiera de las variables independientes. Por ejemplo, estas son las estadísticas de nuestras variables independientes a lo largo de las dos clases, clase 1, "default":

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,independent_variables]),2))
```

y clase 0, "no default":

```{r echo=FALSE}
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,independent_variables]),2))
```

El propósito de dicho análisis de clases es obtener una idea inicial o una intuición de si las clases son en efecto separables, así como entender cuáles de las variables independientes tienen más poder discriminativo.

Notemos sin embargo que:

> Aunque cada variable independiente podría no diferir entre clases, la clasificación podrían aún ser posible: una combinación (lineal o no lineal) de variables independientes podría aún ser discriminativa.

Una simple herramienta de visualización que puede evaluar el poder discriminativo de las variables independientes es el **box plot** (diagrama de caja). Esa visualización representa simple resumen de las estadísticas de una variable independiente (media, mediana, máximo, mínimo, cuantiles, etc.) Por ejemplo, considere el box plot de nuestros datos para las variables de estatus de pago en la clase 1: 

```{r echo=FALSE, fig.height=4.5}
# Please ENTER the selected independent variables for which to draw box plots. 
# Please use numbers, not column names. E.g., c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8.
boxplots_independent_variables = c(7:12) # use only the PAY_ variables
boxplots_independent_variables = c(5:8) # adjust for feature engineering changes

DVvalues = unique(estimation_data[,dependent_variable])
x0 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[1]),boxplots_independent_variables]
x1 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[2]),boxplots_independent_variables]
colnames(x0) <- 1:ncol(x0)
colnames(x1) <- 1:ncol(x1)

swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x1))))
ggplot(melt(cbind.data.frame(n=1:nrow(x1), x1), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x1)))
set_swatch(swatch.default)
```

y la clase 0:

```{r echo=FALSE, fig.height=4.5}
swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x0))))
ggplot(melt(cbind.data.frame(n=1:nrow(x0), x0), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x0)))
set_swatch(swatch.default)
```

**Preguntas:**

1. Dibuje los "box plot"" de las clases 1 y clase 0 para otro conjunto de variables independientes de su elección.
2. ¿Cuáles variables independientes parecen tener el mayor poder discriminativo?

**Respuestas:**

*
*


## Paso 4: Clasificación e Interpretación
Una vez se decide cuáles variables dependientes e independientes a utilizar (las cuáles pueden ser revisadas en iteraciones posteriores), uno puede usar un gran número de métodos de clasificación para desarrollar un modelo que pueda separar ambas clases.

> Algunos de los métodos de clasificación utilizados ampliamente son: árboles de clasificación y regresión (CART), boosted trees, support vector machines, redes neuronales, k vecinos más cercanos (KNN), regresión logística, lasso, bosques aleatorios, deep learning methods, etc. 

```{r echo=FALSE, results='asis'}
cat("In this report we consider only the following classification methods:",paste(Reduce(paste,sapply(head(which(methods_short==1),-1), function(i) paste(names_of_methods_short[i],",",sep=""))),paste(names_of_methods_short[tail(which(methods_short==1),1)],".",sep=""),sep=" "), "Understanding how these methods work is beyond the scope of this note - there are many references available online for all these classification methods.")
```

```{r echo=FALSE}
# We first turn the data into data.frame's
estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)
```

```{r echo=FALSE, eval=logreg, results='asis'}
cat("**Logistic Regression**

These are the estimated parameters for logistic regression on our data:")
```

```{r echo=FALSE, warning=FALSE}
if (logreg){
formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~") # When drop is FALSE, the dimensions of the object are kept. head(x,-1) returns all but the last element of x.

logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)
log_coefficients <- summary(logreg_solution)$coefficients

knitr::kable(log_coefficients)
}
```

```{r echo=FALSE, eval=logreg, results='asis'}
cat("The estimated probability that a validation observation belongs to class 1 (e.g., the estimated probability that the customer defaults) for the first few validation observations, using the logistic regression above, is:")
```

```{r echo=FALSE}
if (logreg){
# Let's get the probabilities for the 3 types of data from the logistic regression
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

# Let's get the decision of the logistic regression for the 3 types of data 
estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_log,validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
}
```

```{r echo=FALSE, eval=logreg, results='asis'}
cat("The default decision is to classify each observation in the group with the highest probability - but one can change this choice.")
```



```{r echo=FALSE, eval=reg_logreg, results='asis'}
cat("**Regularized Logistic Regression**

These are the estimated parameters for regularized logistic regression on our data:")
```

```{r echo=FALSE}
if (reg_logreg){
# Convert the data to matrix format for glmnet functions
est_data <- as.matrix(estimation_data)
cv.out <- cv.glmnet(est_data[,independent_variables],estimation_data[,dependent_variable],alpha=1,family="binomial",type.measure = "deviance") 
#family= binomial(link="logit") => logistic regression
#type.measure is the loss to use for cross-validation. Check https://www.rdocumentation.org/packages/glmnet/versions/2.0-10/topics/cv.glmnet to explore other type.measure options
# alpha controls the kind of regularization: alpha = 1 will penalize the sum of the absolute values of the coefficients - this is known as lasso regression. alpha = 0 will penalize the sum of squares of the coefficients - this is known as ridge regression. Values of alpha between 0 and 1 will do a combination of the above.
# Alternatively, we can do cross-validation on both the lambda (i.e., how much to penalize complex models) and on alpha (i.e., how to capture complexity in model): let the data decide what alpha to use.
# cv.out <- cva.glmnet(est_data[,independent_variables],est_data[,dependent_variable],family="binomial",type.measure = "mse")

#plot result
plot(cv.out)

# Which lambda to use? A natural choice is the lambda that minimizes the mean cross-validated error.
# lambda <- cv.out$lambda.min
# Another choice is to emphasize (more) simplicity, while of course still ensuring good accuracy: choose the largest lambda such that the cross-validated error is within 1 standard error of the minumum error.
lambda <- cv.out$lambda.1se

log_reg_coefficients <- as.matrix(coef(cv.out,s=lambda))
colnames(log_reg_coefficients) <- c("Estimate")
knitr::kable(log_reg_coefficients)
}
```

```{r echo=FALSE, eval=reg_logreg, results='asis'}
cat("The estimated probability that a validation observation belongs to class 1 (e.g., the estimated probability that the customer defaults) for the first few validation observations, using regularized logistic regression above, is:")
```
  
```{r echo=FALSE}
if (reg_logreg){
# Let's get the probabilities for the 3 types of data from the regularized logistic regression
estimation_Probability_class1_reg_log<-as.vector(predict(cv.out, newx=as.matrix(estimation_data[,independent_variables]), s=lambda, type="response"))
validation_Probability_class1_reg_log<-as.vector(predict(cv.out, newx=as.matrix(validation_data[,independent_variables]), s=lambda, type="response"))
test_Probability_class1_reg_log<-as.vector(predict(cv.out, newx=as.matrix(test_data[,independent_variables]), s=lambda, type="response"))

# Let's get the decision of the regularized logistic regression for the 3 types of data 
estimation_prediction_class_reg_log=1*as.vector(estimation_Probability_class1_reg_log > Probability_Threshold)
validation_prediction_class_reg_log=1*as.vector(validation_Probability_class1_reg_log > Probability_Threshold)
test_prediction_class_reg_log=1*as.vector(test_Probability_class1_reg_log > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_reg_log,validation_Probability_class1_reg_log)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
}
```

```{r echo=FALSE, eval=cart, results='asis'}
cat('The table above assumes that the **probability threshold** for considering an observation as "class 1" is ', Probability_Threshold, '.') 
```



```{r echo=FALSE, eval=cart, results='asis'}
cat("**CART**

Running a basic CART model with complexity control ",CART_cp, " leads to the following tree: (**NOTE**: for better readability of the tree figures below,  we will rename the independent variables as IV1 to ", paste("IV", length(independent_variables), sep="")," when using CART):")
```

```{r echo=FALSE}
if (cart){
# Name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
}
```

```{r echo=FALSE, eval=cart, results='asis'}
cat('The leaves of the tree indicate the number of estimation data observations that "reach that leaf" that belong to each class. A perfect classification would only have data from one class in each of the tree leaves. However, such a perfect classification of the estimation data would most likely not be able to classify well out-of-sample data due to overfitting of the estimation data.')
```

```{r echo=FALSE}
# Tree parameter
# Please ENTER the new tree (CART) complexity control cp 
CART_cp = 0.00068
```

```{r echo=FALSE, eval=cart, results='asis'}
cat("One can estimate larger trees through changing the tree's **complexity control** parameter (in this case the rpart.control argument cp). For example, this is how the tree would look like if we set cp=", toString(CART_cp),":")
```

```{r echo=FALSE, warning=FALSE}
if (cart){
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = CART_cp))
rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
}
```

```{r echo=FALSE, eval=cart, results='asis'}
cat("In our case, the probability our validation data belong to class 1 (i.e., a customer's likelihood of default) for the first few validation observations, using the first CART above, is:")
```

```{r echo=FALSE}
if (cart) {
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]

estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_tree,validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_prediction_class_tree_large,validation_Probability_class1_tree_large)
rownames(Classification_Table_large)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
}
```

```{r echo=FALSE, eval=cart, results='asis'}
cat('The table above assumes that the **probability threshold** for considering an observation as "class 1" is ', Probability_Threshold, '.') 
```



<!-- **Random Forest:** -->
```{r echo=FALSE}
# model_forest <- randomForest(x=estimation_data[,independent_variables],y=estimation_data[,dependent_variable], importance=TRUE, proximity=TRUE, type="classification")
# 
# # Let's get the probabilities for the 3 types of data from the random forest
# estimation_Probability_class1_random_forest<-predict(model_forest, estimation_data, type="response")
# validation_Probability_class1_random_forest<-predict(model_forest, validation_data, type="response")
# test_Probability_class1_random_forest<-predict(model_forest, test_data, type="response")
# 
# # Let's get the decision of the random forest for the 3 types of data 
# estimation_prediction_class_random_forest=1*as.vector(estimation_Probability_class1_random_forest > Probability_Threshold)
# validation_prediction_class_random_forest=1*as.vector(validation_Probability_class1_random_forest > Probability_Threshold)
# test_prediction_class_random_forest=1*as.vector(test_Probability_class1_random_forest > Probability_Threshold)
# 
# Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_random_forest,validation_Probability_class1_random_forest)
# rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
# colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")
# 
# knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
```



```{r echo=FALSE, eval=XGBoost, results='asis'}
cat("**XGBoost**

The estimated probability that a validation observation belongs to class 1 (e.g., the estimated probability that the customer defaults) for the first few validation observations, using XGBoost, is:")
```

```{r echo=FALSE}
if (XGBoost){
model_xgboost <- xgboost(data = as.matrix(estimation_data[,independent_variables]), label = estimation_data[,dependent_variable], eta = 0.3, max_depth = 10, nrounds=10, objective = "binary:logistic", verbose = 0)
# eta: step size of each boosting step. max.depth: maximum depth of the tree. nrounds: the max number of iterations. We use the logistic regression as the objective. verbose=1 prints information about performance. Check https://www.rdocumentation.org/packages/xgboost/versions/0.4-4/topics/xgboost

# # Alternative way of building the model, recommended when there are factors among the variables (xgboost does not accept factors as inputs, so need to use a design matrix):
# formula_allvariables=paste(colnames(estimation_data[,dependent_variable,drop=F]),".",sep="~")
# training.x <-model.matrix(as.formula(formula_allvariables), data = estimation_data)
# testing.x <-model.matrix(as.formula(formula_allvariables), data = validation_data)
# 
# model_xgboost<-xgboost(data = data.matrix(training.x[,-1]), label = estimation_data[,dependent_variable], eta = 0.3, max_depth = 10, nround=10, objective = "binary:logistic", verbose = 1)
# 
# xgboost_prediction<-predict(model_xgboost,newdata=testing.x[,-1], type="response")
# #confusionMatrix(ifelse(XGboost_prediction>0.5,1,0),validation_data$default.payment.next.month) #Display confusion matrix (needs caret package)

# Let's get the probabilities for the 3 types of data from xgboost
estimation_Probability_class1_xgboost<-predict(model_xgboost, newdata=as.matrix(estimation_data[,independent_variables]), type="response")
validation_Probability_class1_xgboost<-predict(model_xgboost, newdata=as.matrix(validation_data[,independent_variables]), type="response")
#confusionMatrix(ifelse(validation_Probability_class1_xgboost>0.5,1,0),validation_data[,dependent_variable]) #Display confusion matrix (needs caret package)
test_Probability_class1_xgboost<-predict(model_xgboost, newdata=as.matrix(test_data[,independent_variables]), type="response")

# Let's get the decision of xgboost for the 3 types of data 
estimation_prediction_class_xgboost=1*as.vector(estimation_Probability_class1_xgboost > Probability_Threshold)
validation_prediction_class_xgboost=1*as.vector(validation_Probability_class1_xgboost > Probability_Threshold)
test_prediction_class_xgboost=1*as.vector(validation_Probability_class1_xgboost > Probability_Threshold)

Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_xgboost,validation_Probability_class1_xgboost)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
}
```


## Paso 5: Precisión del conjunto de validación.

Utilizando las probabilidades de las predicciones del conjunto de validación, cómo se mencionó antes, se pueden generar medidas de desempeño de la clasificación. Antes de discutirlas, notemos que, dada la probabilidad de pertenecer a una clase, **una opción de predicción razonable sería simplemente predecir la clase que tenga la probabilidad de ocurrencia más alta (o la más frecuente)**. Sin embargo, esta no tiene que ser la única opción en la práctica.

> Seleccionar un umbral de probabilidad para decidir la clase a la que pertenece una observación es una decisión importante que el usuario debe hacer. Mientras en algunos casos una probabilidad razonable es 50%, en otros debería ser 99.9% o 0.01%. ¿pueden pensar acerca de esos casos?

**Preguntas:**

1. Puede pensar sobre tal escenario:

**Respuestas:**

*

Para diferentes opciones de umbral de probabilidad, uno puede medir varias métricas de desempeño, las cuales se describen a continuación.

### 1.  Proporción de aciertos (Hit ratio)
Es simplemente el porcentaje de observaciones que han sido correctamente clasificadas (la clase predecida es la clase real). Podemos simplemente contar el número de datos de validación clasificados correctamente y dividir ese número entre el número total de observaciones del conjunto de validación, utilizando dos CART y una regresión logística. Los siguientes son los porcentajes de acierto utilizando un umbral de probabilidad de `r Probability_Threshold*100`%:


```{r echo=FALSE}
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_log,
                               validation_prediction_class_tree,
                               validation_prediction_class_tree_large,
                               validation_prediction_class_reg_log,
                               validation_prediction_class_xgboost)
validation_hit_rates = rbind(
  100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual),
  100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual),
  100*sum(validation_prediction_class_reg_log==validation_actual)/length(validation_actual),
  100*sum(validation_prediction_class_xgboost==validation_actual)/length(validation_actual)
  )
validation_hit_rates = as.matrix(validation_hit_rates[which(methods==1)])

colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- names_of_methods[which(methods==1)]
knitr::kable(validation_hit_rates)
```
Mientras que para los datos de estimación, las tasas de precisión son:

```{r echo=FALSE}
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_log,
                               estimation_prediction_class_tree,
                               estimation_prediction_class_tree_large,
                               estimation_prediction_class_reg_log,
                               estimation_prediction_class_xgboost
                               )
estimation_hit_rates = rbind(
  100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual),
  100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual),
  100*sum(estimation_prediction_class_reg_log==estimation_actual)/length(estimation_actual),
  100*sum(estimation_prediction_class_xgboost==estimation_actual)/length(estimation_actual)
  )
estimation_hit_rates = as.matrix(estimation_hit_rates[which(methods==1)])

colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- names_of_methods[which(methods==1)]
knitr::kable(estimation_hit_rates)
```

Un simple punto de referencia para comparar el desempeño de un modelo de clasificación es el **criterio de la máxima oportunidad**. Este mide la proporción de la clase más grande. Para nuestros datos de validación el grupo más grande son las personas que no entran en default: `r sum(!validation_actual)` de `r length(validation_actual)` personas. Claramente, si clasificamos todos los individuos dentro del grupo más grande, podemos tener una tasa de aciertos del `r round(100*sum(!validation_actual)/length(validation_actual), 2)`%, sin hacer ningún trabajo. Uno puede alcanzar al menos una tasa de aciertos de al menos la máxima oportunidad, aunque como veremos más adelante aún hay más criterios de desempeño a considerar.


### 2. Matriz de Confusión.

La matriz de confusión muestra el número de datos que ha sido clasificado correctamente para cada clase. Por ejemplo, para el método con la tasa de aciertos más alta en los datos de validación (entre la regresión logística y los dos modelos CART), y para un umbral de probabilidad del `r Probability_Threshold*100`%, la matriz de confusión es:


```{r echo=FALSE}
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(validation_prediction_best*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!validation_prediction_best)*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```

**Preguntas:**

1. Note que los porcentajes suman 100% para cada fila. ¿Por qué?
2. Además, una "buena" matriz de confusión debe tener grandes valores en su diagonal y pequeños valores en las otras posiciones. ¿Por qué?

**Respuestas:**

*
*



### 3. Curva ROC.

Recuerde que cada observación es clasificada por nuestro modelo de acuerdo a las probabilidades Pr(0) y Pr(1) y un determinado umbral de probabilidad. Comúnmente se establece el umbral de probabilidad en 0.5 (así que las observaciones con Pr(1)>0.5 son clasificadas como 1). Sin embargo, podemos variar este umbral, por ejemplo si estamos interesados en clasificar correctamente todos los 1's pero no nos interesa perder los 0's (y viceversa).

Cuando cambiamos el umbral de probabilidad obtenemos diferentes valores de proporción de aciertos, falsos positivos, falsos negativos, o cualquier otra medida de desempeño. Podemos graficar, por ejemplo, cómo cambian los falsos positivos vs los verdaderos positivos a medida cambiamos el umbral de probabilidad, y generamos la curva ROC.

Las curvas ROC para los datos de validación para ambos CARTs arriba así como para la regresión logística son:

```{r echo=FALSE}
validation_actual_class <- as.numeric(validation_data[,dependent_variable])

pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)
pred_reg_log <- prediction(validation_Probability_class1_reg_log, validation_actual_class)
pred_xgboost <- prediction(validation_Probability_class1_xgboost, validation_actual_class)

test1<-performance(pred_tree, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive rate CART 1")
plot1 <- ggplot(df1, aes(x=`False Positive rate CART 1`, y=`True Positive rate CART 1`)) + geom_line()
test2<-performance(pred_log, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate log reg", "True Positive rate log reg")
plot2 <- ggplot(df2, aes(x=`False Positive rate log reg`, y=`True Positive rate log reg`)) + geom_line()
test3<-performance(pred_tree_large, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate CART 2", "True Positive rate CART 2")
plot3 <- ggplot(df3, aes(x=`False Positive rate CART 2`, y=`True Positive rate CART 2`)) + geom_line()
test4<-performance(pred_reg_log, "tpr", "fpr")
df4<- cbind(as.data.frame(test4@x.values),as.data.frame(test4@y.values))
colnames(df4) <- c("False Positive rate regularized log reg", "True Positive rate regularized log reg")
plot4 <- ggplot(df4, aes(x=`False Positive rate regularized log reg`, y=`True Positive rate regularized log reg`)) + geom_line()
test5<-performance(pred_xgboost, "tpr", "fpr")
df5<- cbind(as.data.frame(test5@x.values),as.data.frame(test5@y.values))
colnames(df5) <- c("False Positive rate XGBoost", "True Positive rate XGBoost")
plot5 <- ggplot(df5, aes(x=`False Positive rate XGBoost`, y=`True Positive rate XGBoost`)) + geom_line()

# We can plot the curves individually 
# grid.arrange(plot1, plot2, plot3, plot4, plot5)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
list_of_df <- list(df2, df1, df3, df4, df5)
df.all <- do.call(rbind, lapply(list_of_df[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive rate ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

¿Cómo debería verse una buena curva ROC? La regla básica para evaluar una curva ROC es que entre más "alta" mejor, porque el área bajo la curva es más grande. Usted podría también seleccionar un punto en la curva ROC (el mejor para nuestro propósito) y utilizar que el desempeño de los falsos positivos y falsos negativos (y su correspondiente umbral para P(1)) para evaluar el modelo.

**Preguntas:**

1. ¿Que punto en la curva ROC seleccionaría?
2. ¿A qué clasificador corresponde la línea punteada de 45°? ¿Cómo muestra la curva ROC que tanto la regresión logística como modelos CART son superiores a dicho clasificador?


**Respuestas:**

*
*

### 4. Curva Gains
La curva Gains es una técnica popular en ciertas aplicaciones, cómo en marketing o riesgo de crédito. 

Como un ejemplo concreto, considere el caso de una campaña de marketing por correo. Digamos que tenemos un clasificadorque intenta identificar las personas que si responderán a la encuesta. Nosotros quisieramos seleccionar los menos casos posibles y aún así capturar el máximo número de encuestas posibles.

Podemos medir el porcentaje de todas las respuestas que el clasificador captura, digamos, x% de los casos: el mejor x% en términos de probabilidad de respuesta asignada por nuestro clasificador. Para cada porcentaje de los casos que seleccionamos (x), podemos graficar el siguiente punto: el eje-x será el porcentaje de todos los casos que fueron seleccionados, mientras que el eje-y será el porcentaje de todos los casos pertenecientes a la clase 1 de los que fueron seleccionados (la proporción de verdaderos positivos/ predecidos positivos de nuestro clasificador, asumiendo que el clasificador predice clase 1 para todos los casos, y predice clase 0 para los restantes). Si graficamos esos puntos mientras cambiamos el porcentaje de cases seleccionados (x) (mientras se cambia el umbral de probabilidad del clasificador), obtenemos una gráfica llamada la **curva gains**. 

En el caso de default de tarjetas de crédito, la curva gains para los datos de validación de nuestros tres clasificadores son las siguientes:

```{r echo=FALSE}
validation_actual <- validation_data[,dependent_variable]
all1s <- sum(validation_actual); 

probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % of validation data`, y=`CART 1 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % of validation data`, y=`CART 2 % of class 1`)) + geom_line()

probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % of validation data`, y=`log reg % of class 1`)) + geom_line()

probs <- validation_Probability_class1_reg_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame4 <- data.frame(
  `regularized log reg % of validation data` = res[1,],
  `regularized log reg % of class 1` = res[2,],
  check.names = FALSE
)
plot4 <- ggplot(frame4, aes(x=`regularized log reg % of validation data`, y=`regularized log reg % of class 1`)) + geom_line()

probs <- validation_Probability_class1_xgboost
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s) 
}))
frame5 <- data.frame(
  `XGBoost % of validation data` = res[1,],
  `XGBoost % of class 1` = res[2,],
  check.names = FALSE
)
plot5 <- ggplot(frame5, aes(x=`XGBoost % of validation data`, y=`regularized log reg % of class 1`)) + geom_line()

# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3, plot4)   # use `fig.height=7.5` for the grid plot

# But we are going to combine them instead
list_of_frames <- list(frame3, frame1, frame2, frame4, frame5)
df.all <- do.call(rbind, lapply(list_of_frames[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of validation data selected"
  df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Note que si fuéramos a examinar casos aleatorios, en lugar de seleccionar los "mejores" usando un clasificador informado, la "predicción aleatoria" de la curva gains sería una línea recta de 45 grados.

**Pregunta:**

¿Por qué?

**Respuesta:**

*
¿Cómo debería verse una buena curva gains? Entre más arriba de la línea de 45° esté nuestra curva gains, mejor. Además, así como la curva ROC, uno puede seleccionar el porcentaje de los casos de todos los casos a examinar apropiadamente de modo que en cualquier punto de la curva gains sea seleccionado.

**Pregunta:**

¿Cuál punto de la curva gains deberíamos seleccionar en la práctica?

**Respuesta:**

*

### 5. Curva de Ganancias

Finalmente, podemos generar la llamada curva de ganancias, la cual se utiliza frecuentemente para tomar la decisión final. Considere una campaña directa de marketing, y suponga el $costo de enviar publicidad como 1, y una ganancia esperada de una persona que responde positivamente de $45. Suponga que tiene una base de datos de 1 millón de personas a las que se les puede potencialmente enviar publicidad.  Las típicas tasas de respuesta son 0.05%. ¿Qué fracción del millón de personas debería enviarle la publicidad? 

Para responder este tipo de preguntas necesitamos crear la **curva de ganancias**. Podemos medir cierta métrica de ganancias si solo seleccionamos los mejores casos en términos de probabilidad de respuesta asignada por nuestro clasificador. Luego podemos graficar la curva de ganancias mientras cambiamos el porcentaje de casos seleccionados así como hicimos con la curva gains, y calculando la correspondiente **ganancia estimada** (o pérdida). Esto es simplemente igual a:

> Ganancia total esperada = (% de 1's correctamente predecidos) x (valor de capturar un 1) + (% de 0's correctamente predecidos) x (valor de capturar un 0) + (% de 1's incorrectamente predecidos como 0) x (valor de perder un 1) + (% de 0's incorrectamente predecidos como 1) x (valor de perder un 0)

> Calcular la ganancia esperada requiere de tener una estimación de los cuatro valores: valor de capturar un 0 o 1, y costo de perder un 0 o 1 o viceversa.

Dados los valores y costos de clasificar correcta e incorrectamente, podemos graficar la ganancia total esperada (o pérdida) a medida cambiamos el porcentaje de casos seleccionados, por ejemplo cuando usamos el umbral de probabilidad como cuando generamos las curvas ROC.

En el caso de riesgo de crpedito, nosotros consideramos las siguientes pérdidas y ganancias por clasificar a los clientes:

```{r echo=FALSE}
knitr::kable(Profit_Matrix)
```

Basado en esos estimados de ganancias y pérdidas, las curvas de ganancias para los datos de validación de los clasificadores son:

```{r echo=FALSE}
actual_class <- validation_data[,dependent_variable]

probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame1 <- data.frame(
  `CART 1 % selected` = res[1,],
  `CART 1 est. profit` = res[2,],
  check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % selected`, y=`CART 1 est. profit`)) + geom_line()

probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame2 <- data.frame(
  `CART 2 % selected` = res[1,],
  `CART 2 est. profit` = res[2,],
  check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % selected`, y=`CART 2 est. profit`)) + geom_line()

probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame3 <- data.frame(
  `log reg % selected` = res[1,],
  `log reg est. profit` = res[2,],
  check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % selected`, y=`log reg est. profit`)) + geom_line()

probs <- validation_Probability_class1_reg_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame4 <- data.frame(
  `regularized log reg % selected` = res[1,],
  `regularized log reg est. profit` = res[2,],
  check.names = FALSE
)
plot4 <- ggplot(frame4, aes(x=`regularized log reg % selected`, y=`regularized log reg est. profit`)) + geom_line()

probs <- validation_Probability_class1_xgboost
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame5 <- data.frame(
  `xgboost % selected` = res[1,],
  `xgboost est. profit` = res[2,],
  check.names = FALSE
)
plot5 <- ggplot(frame5, aes(x=`XGBoost % selected`, y=`XGBoost est. profit`)) + geom_line()

# we can plot the curves individually 
# grid.arrange(plot1, plot2, plot3, plot4, plot5)   # use `fig.height=7.5` for the grid plot

# But we're going to combine them instead
list_of_frames <- list(frame3, frame1, frame2, frame4, frame5)
df.all <- do.call(rbind, lapply(list_of_frames[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" est. profit", "", df$variable)
  colnames(df)[1] <- "% of validation data selected"
  df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
```

Podemos ahora seleccionar el porcentaje de casos seleccionados que corresponde la máxima ganancia estimada (o pérdida mínima, si es necesario).


**Pregunta:**

¿Qué punto de la curva de ganancia debería seleccionar en la práctica?

**Respuesta:**

*

Notemos que para maximizar la ganancia esperada necesitamos tener una estimación de la ganancia o pérdida para cada uno de los cuatro escenarios! Esto puede ser difícil de evaluar, por lo que normalmente se necesita realizar cierto análisis de sensibilidad a las ganancias y pérdidas que han sido asumidas. Por ejemplo, podemos generar diferentes curvas de ganancia (el peor caso, el mejor caso, y el caso promedio) y ver cuánto varía la mejor ganancia, y más importante ver **como varía nuestro modelo de clasificación y umbral de probabilidad correspondiente a la mejor ganancia**, ya que es lo que eventualmente debemos decidir.


## Paso 6: Precisión del modelo

Habiendo iterado los pasos 2-5 hasta que estemos satisfechos con el desempeño del modelo en los datos de validación, en este paso necesitamos hacer el análisis del desempeño hecho en el paso 5 para los datos de evaluación (test set). Esta es la medida de desempeño que mejor representa lo que uno esperaría en la práctica una vez implementada la solución, **asumiendo (como siempre) que los datos utilizados en el análisis son representativos de la situación en la que será implementado el sistema.**

Veamos en nuestro caso la **hit ratio, matriz de confusión, curva ROC, curva gains, y curva de ganancias** para los datos de evaluación. Para el the hit ratio y la matriz de confusión usamos `r Probability_Threshold*100`% como el umbral de probabilidad de la clasificación.

```{r echo=FALSE}
######for train data#####
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_log,
                         test_prediction_class_tree,
                         test_prediction_class_tree_large,
                         test_prediction_class_reg_log,
                         test_prediction_class_xgboost
                         )
test_hit_rates = rbind(
  100*sum(test_prediction_class_log==test_actual)/length(test_actual),
  100*sum(test_prediction_class_tree==test_actual)/length(test_actual), 
  100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual),
  100*sum(test_prediction_class_reg_log==test_actual)/length(test_actual),
  100*sum(test_prediction_class_xgboost==test_actual)/length(test_actual)
  )
test_hit_rates = as.matrix(test_hit_rates[which(methods==1)])

colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- names_of_methods[which(methods==1)]
knitr::kable(test_hit_rates)
```

La matriz de confusión para el modelo con la mejor proporción de aciertos en nuestros datos de validación:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
test_prediction_best = test_predictions[which.max(test_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
```

Curva ROC para los datos de evaluación:

```{r echo=FALSE}
test_actual_class <- as.numeric(test_data[,dependent_variable])

pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)
pred_reg_log_test <- prediction(test_Probability_class1_reg_log, test_actual_class)
pred_xgboost_test <- prediction(test_Probability_class1_xgboost, test_actual_class)

test<-performance(pred_tree_test, "tpr", "fpr")
df1<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive CART 1")
test2<-performance(pred_tree_large_test, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART 2", "True Positive CART 2")
test3<-performance(pred_log_test, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate log reg", "True Positive log reg")
test4<-performance(pred_reg_log_test, "tpr", "fpr")
df4<- cbind(as.data.frame(test4@x.values),as.data.frame(test4@y.values))
colnames(df4) <- c("False Positive rate regularized log reg", "True Positive regularized log reg")
test5<-performance(pred_xgboost_test, "tpr", "fpr")
df5<- cbind(as.data.frame(test5@x.values),as.data.frame(test5@y.values))
colnames(df5) <- c("False Positive rate XGBoost", "True Positive XGBoost")

list_of_df <- list(df3, df1, df2, df4, df5)
df.all <- do.call(rbind, lapply(list_of_df[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub("True Positive ", "", df$variable)
  colnames(df)[1] <- "False Positive rate"
  df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Curva Gains para los datos de evaluación:

```{r echo=FALSE}
test_actual <- test_data[,dependent_variable]
all1s <- sum(test_actual)

probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame1 <- data.frame(
  `CART 1 % of validation data` = res[1,],
  `CART 1 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
}))
frame2 <- data.frame(
  `CART 2 % of validation data` = res[1,],
  `CART 2 % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
frame3 <- data.frame(
  `log reg % of validation data` = res[1,],
  `log reg % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_reg_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
frame4 <- data.frame(
  `regularized log reg % of validation data` = res[1,],
  `regularized log reg % of class 1` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_xgboost
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
frame5 <- data.frame(
  `XGBoost % of validation data` = res[1,],
  `XGBoost % of class 1` = res[2,],
  check.names = FALSE
)

list_of_frames <- list(frame3, frame1, frame2, frame4, frame5)
df.all <- do.call(rbind, lapply(list_of_frames[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" % of class 1", "", df$variable)
  colnames(df)[1] <- "% of test data selected"
  df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
```

Finalmente, curva de ganancias para los datos de evaluación, usando las mismas estimaciones de pérdidas y ganancias que hicimos arriba:

```{r echo=FALSE}
actual_class<- test_data[,dependent_variable]

probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame1 <- data.frame(
  `CART 1 % selected` = res[1,],
  `CART 1 est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame2 <- data.frame(
  `CART 2 % selected` = res[1,],
  `CART 2 est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame3 <- data.frame(
  `log reg % selected` = res[1,],
  `log reg est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_reg_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame4 <- data.frame(
  `regularized log reg % selected` = res[1,],
  `regularized log reg est. profit` = res[2,],
  check.names = FALSE
)

probs <- test_Probability_class1_xgboost
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
  useonly <- which(probs >= prob)
  predict_class <- 1*(probs >= prob)
  theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
    Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
    Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
    Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
  c(100*length(useonly)/length(actual_class), theprofit) 
}))
frame5 <- data.frame(
  `XGBoost % selected` = res[1,],
  `XGBoost est. profit` = res[2,],
  check.names = FALSE
)

list_of_frames <- list(frame3, frame1, frame2, frame4, frame5)
df.all <- do.call(rbind, lapply(list_of_frames[which(methods==1)], function(df) {
  df <- melt(df, id=1)
  df$variable <- sub(" est. profit", "", df$variable)
  colnames(df)[1] <- "% of test data selected"
  df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
```

**Preguntas:**

1. ¿Es el desempeño en los datos de evaluación similar al desempeño en los datos de validación? ¿Deberíamos esperar que el desempeño de nuestra clasificación sea cercano al que se obtienen en los datos de evaluación cuando se implementa en producción? ¿Porqué o porqué no? ¿Que deberíamos hacer si son diferentes?
2. Haga una evaluación final acerca de cuál clasificador debería ser usado (de los considerados aquí) para la clasificación de default en tarjetas de crédito, con qué porcentaje de casos/umbral de probabilidad, y diga la razón. ¿Cuál es la ganancia que la compañía podría lograr (como se midió con los datos de evaluación) basado en su solución?
3. ¿Qué tanto depende su evaluación de los valores de correcta o incorrectamente clasificar (`r actual_1_predict_1`, `r actual_1_predict_0`, `r actual_0_predict_1`, `r actual_0_predict_0`)?
4. ¿Cuál decisión de negocios puede hacer el emisor de tarjetas de crédito basado en este análisis?


**Respuestas:**

*
*
*
*