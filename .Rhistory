datafile_name = "../../Data/UCI_Credit_Card.csv"
ProjectData <- read.csv(datafile_name)
# Convertimos los datos a la clase data.matrix para que sea mas fácil de manipular
ProjectData <- data.matrix(ProjectData)
# Por favor INGRESE la variable dependiente(clase).
# Por favor use números, no nombres de columnas. Por ejemplo, 82 usa la columna en la posición 82 como la variable dependiente.
# Necesita asegurarse que la variable dependiente solo tome dos valores: 0 y 1.
dependent_variable = 25
# Por favor ingrese los atributos a usar como variables independientes.
# Por favor use números, no nombres de columnas. Por ejemplo, c(1:5, 7, 8) usa las columnas 1,2,3,4,5,7,8.
independent_variables = c(1:24) # usa todos los atributos disponibles
dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectData), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectData), max(i,1))))
if (length(unique(ProjectData[,dependent_variable])) !=2){
cat("\n*****\n REVISE DE NUEVO, LA VARIABLE DEPENDIENTE TOMA MÁS DE 2 VALORES")
cat("\nDividiendo los datos respecto a la mediana...\n*****\n ")
new_dependent = ProjectData[,dependent_variable] >= median(ProjectData[,dependent_variable])
ProjectData[,dependent_variable] <- 1*new_dependent
}
# Por favor INGRESE el umbral de probabilidad arriba del cual una observación será categorizada como clase 1:
Probability_Threshold = 0.5 # entre 0 y 1
# Por favor INGRESE el porcentaje de datos usados para la estimación
estimation_data_percent = 80
validation_data_percent = 10
test_data_percent = 100-estimation_data_percent-validation_data_percent
# Por favor INGRESE 1 si quiere dividir los datos aleatoriamente en los conjuntos de estimación y validación/evaluación
random_sampling = 0
# Parámetros del árbol
# Por favor INGRESE el control de complejidad cp del árbol(CART), por ejemplo de 0.0001 a 0.02, dependiendo de los datos.
CART_cp = 0.0025
CART_control = rpart.control(cp = CART_cp)
# Por favor INGRESE el significado de las clases 1 y 0:
class_1_interpretation = "default"
class_0_interpretation = "no default"
# Por favor INGRESE los valores de ganancia/costo de clasificar correcta o incorrectamente los datos:
actual_1_predict_1 = 0
actual_1_predict_0 = -100000
actual_0_predict_1 = 0
actual_0_predict_0 = 20000
Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix) <- c(paste("Predict 1 (", class_1_interpretation, ")", sep = ""), paste("Predict 0 (", class_0_interpretation, ")", sep = ""))
rownames(Profit_Matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
# Por favor INGRESE el máximo número de observaciones a mostrar en el reporte y diapositivas
# (El número predefinido es 50. Si el número es demasiado grande el reporte y las diapositivas podrían no ser generados -  muy lento o no funcionará!!)
max_data_report = 10
knitr::kable({
df <- t(head(round(ProjectData[,independent_variables],2), max_data_report))
colnames(df) <- sprintf("%02d", 1:ncol(df))
df
})
if (random_sampling){
estimation_data_ids=sample.int(nrow(ProjectData),floor(estimation_data_percent*nrow(ProjectData)/100))
non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids) #setdiff(x,y) returns the elements of x that are not in y
validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
} else {
estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectData)/100)
non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids)
validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
}
test_data_ids = setdiff(1:nrow(ProjectData), union(estimation_data_ids,validation_data_ids))
estimation_data=ProjectData[estimation_data_ids,]
validation_data=ProjectData[validation_data_ids,]
test_data=ProjectData[test_data_ids,]
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
knitr::kable(class_percentages)
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,independent_variables]),2))
knitr::kable(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,independent_variables]),2))
# Please ENTER the selected independent variables for which to draw box plots.
# Please use numbers, not column names. E.g., c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8.
boxplots_independent_variables = c(7:12) # use only the PAY_ variables
DVvalues = unique(estimation_data[,dependent_variable])
x0 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[1]),boxplots_independent_variables]
x1 = estimation_data[which(estimation_data[,dependent_variable]==DVvalues[2]),boxplots_independent_variables]
colnames(x0) <- 1:ncol(x0)
colnames(x1) <- 1:ncol(x1)
swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x1))))
ggplot(melt(cbind.data.frame(n=1:nrow(x1), x1), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x1)))
set_swatch(swatch.default)
swatch.default <- as.character(swatch())
set_swatch(c(swatch.default[1], colorRampPalette(RColorBrewer::brewer.pal(12, "Paired"))(ncol(x0))))
ggplot(melt(cbind.data.frame(n=1:nrow(x0), x0), id="n"), aes(x=n, y=value, colour=variable)) + geom_boxplot(fill="#FFFFFF", size=0.66, position=position_dodge(1.1*nrow(x0)))
set_swatch(swatch.default)
# We first turn the data into data.frame's
estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)
formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~") # When drop is FALSE, the dimensions of the object are kept. head(x,-1) returns all but the last element of x.
logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)
log_coefficients <- round(summary(logreg_solution)$coefficients,1)
knitr::kable(round(log_coefficients,2))
# Let's get the probabilities for the 3 types of data from the logistic regression
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])
# Let's get the decision of the logistic regression for the 3 types of data
estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)
Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_log,validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")
knitr::kable(head(t(round(Classification_Table,2)), max_data_report)) #t(x) returns the transpose of x
# Name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")
estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)
validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)
test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)
estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)
formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)
rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
# Tree parameter
# Please ENTER the new tree (CART) complexity control cp
CART_cp = 0.00068
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = CART_cp))
rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]
validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]
test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]
estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)
validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)
test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)
Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_tree,validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")
Classification_Table_large=rbind(validation_data[,dependent_variable],validation_prediction_class_tree_large,validation_Probability_class1_tree_large)
rownames(Classification_Table_large)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")
knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))
tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)
large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)
Importance_table <- cbind(log_importance,tree_importance_final,large_tree_importance_final)
colnames(Importance_table) <- c("Logistic Regression", "CART 1", "CART 2")
rownames(Importance_table) <- rownames(log_importance)
## printing the result in a clean-slate table
knitr::kable(round(Importance_table,2))
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_log,
validation_prediction_class_tree,
validation_prediction_class_tree_large)
validation_hit_rates = rbind(
100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual),
100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual),
100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual)
)
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("Logistic Regression", "First CART", "Second CART")
knitr::kable(validation_hit_rates)
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_log,
estimation_prediction_class_tree,
estimation_prediction_class_tree_large)
estimation_hit_rates = rbind(
100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual),
100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual),
100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual)
)
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(estimation_hit_rates)
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(validation_prediction_best*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!validation_prediction_best)*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)
colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
validation_actual_class <- as.numeric(validation_data[,dependent_variable])
pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)
test1<-performance(pred_tree, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive rate CART 1")
plot1 <- ggplot(df1, aes(x=`False Positive rate CART 1`, y=`True Positive rate CART 1`)) + geom_line()
test2<-performance(pred_log, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate log reg", "True Positive rate log reg")
plot2 <- ggplot(df2, aes(x=`False Positive rate log reg`, y=`True Positive rate log reg`)) + geom_line()
test3<-performance(pred_tree_large, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate CART 2", "True Positive rate CART 2")
plot3 <- ggplot(df3, aes(x=`False Positive rate CART 2`, y=`True Positive rate CART 2`)) + geom_line()
# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
df <- melt(df, id=1)
df$variable <- sub("True Positive rate ", "", df$variable)
colnames(df)[1] <- "False Positive rate"
df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
validation_actual <- validation_data[,dependent_variable]
all1s <- sum(validation_actual);
probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame1 <- data.frame(
`CART 1 % of validation data` = res[1,],
`CART 1 % of class 1` = res[2,],
check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % of validation data`, y=`CART 1 % of class 1`)) + geom_line()
probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame2 <- data.frame(
`CART 2 % of validation data` = res[1,],
`CART 2 % of class 1` = res[2,],
check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % of validation data`, y=`CART 2 % of class 1`)) + geom_line()
probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame3 <- data.frame(
`log reg % of validation data` = res[1,],
`log reg % of class 1` = res[2,],
check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % of validation data`, y=`log reg % of class 1`)) + geom_line()
# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" % of class 1", "", df$variable)
colnames(df)[1] <- "% of validation data selected"
df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
knitr::kable(Profit_Matrix)
actual_class <- validation_data[,dependent_variable]
probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame1 <- data.frame(
`CART 1 % selected` = res[1,],
`CART 1 est. profit` = res[2,],
check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % selected`, y=`CART 1 est. profit`)) + geom_line()
probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame2 <- data.frame(
`CART 2 % selected` = res[1,],
`CART 2 est. profit` = res[2,],
check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % selected`, y=`CART 2 est. profit`)) + geom_line()
probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame3 <- data.frame(
`log reg % selected` = res[1,],
`log reg est. profit` = res[2,],
check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % selected`, y=`log reg est. profit`)) + geom_line()
# we can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we're going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" est. profit", "", df$variable)
colnames(df)[1] <- "% of validation data selected"
df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
######for train data#####
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_log,
test_prediction_class_tree,
test_prediction_class_tree_large
)
test_hit_rates = rbind(
100*sum(test_prediction_class_log==test_actual)/length(test_actual),
100*sum(test_prediction_class_tree==test_actual)/length(test_actual),
100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual)
)
colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(test_hit_rates)
test_prediction_best = test_predictions[which.max(test_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)
colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
test_actual_class <- as.numeric(test_data[,dependent_variable])
pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)
test<-performance(pred_tree_test, "tpr", "fpr")
df1<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive CART 1")
test2<-performance(pred_tree_large_test, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART 2", "True Positive CART 2")
test3<-performance(pred_log_test, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate log reg", "True Positive log reg")
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
df <- melt(df, id=1)
df$variable <- sub("True Positive ", "", df$variable)
colnames(df)[1] <- "False Positive rate"
df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
test_actual <- test_data[,dependent_variable]
all1s <- sum(test_actual)
probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame1 <- data.frame(
`CART 1 % of validation data` = res[1,],
`CART 1 % of class 1` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame2 <- data.frame(
`CART 2 % of validation data` = res[1,],
`CART 2 % of class 1` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame3 <- data.frame(
`log reg % of validation data` = res[1,],
`log reg % of class 1` = res[2,],
check.names = FALSE
)
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" % of class 1", "", df$variable)
colnames(df)[1] <- "% of test data selected"
df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
actual_class<- test_data[,dependent_variable]
probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame1 <- data.frame(
`CART 1 % selected` = res[1,],
`CART 1 est. profit` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame2 <- data.frame(
`CART 2 % selected` = res[1,],
`CART 2 est. profit` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame3 <- data.frame(
`log reg % selected` = res[1,],
`log reg est. profit` = res[2,],
check.names = FALSE
)
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" est. profit", "", df$variable)
colnames(df)[1] <- "% of test data selected"
df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
rmarkdown::render()
rmarkdown::render(input = "Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render(input = "./Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render(input = "../Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render("../Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6Clasificacion")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6Clasificacion/")
rmarkdown::render("../Sesiones/Sesiones_5_6_Clasificacion/Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render(Deafult_Tarjeta_Credito.Rmd)
rmarkdown::render('Deafult_Tarjeta_Credito.Rmd')
rmarkdown::render("../Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render("./Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics")
rmarkdown::render("../Sesiones/Sesiones_5_6_Clasificacion/Deafault_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
rmarkdown::render("Deafault_Tarjeta_Credito.Rmd")
list.files()
rmarkdown::render("Default_Tarjeta_Credito.Rmd")
getwd()
knit("Default_Tarjeta_Credito.Rmd")
knit2html("Default_Tarjeta_Credito.Rmd")
version
getOption(“rstudio.markdownToHTML”)
getOption()
make_pdf_file = 0 # Haga este número 1 si quiere un archivo PDF, 0 para HTML
source("../../AnalyticsLibraries/library.R")
getOption(“rstudio.markdownToHTML”)
getOption("rstudio.markdownToHTML")
