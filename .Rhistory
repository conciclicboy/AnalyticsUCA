# Name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")
estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)
validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)
test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)
estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)
formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)
rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
# Tree parameter
# Please ENTER the new tree (CART) complexity control cp
CART_cp = 0.00068
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = CART_cp))
rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]
validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]
test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]
estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)
validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)
test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)
Classification_Table=rbind(validation_data[,dependent_variable],validation_prediction_class_tree,validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")
Classification_Table_large=rbind(validation_data[,dependent_variable],validation_prediction_class_tree_large,validation_Probability_class1_tree_large)
rownames(Classification_Table_large)<-c("Actual Class","Predicted Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")
knitr::kable(head(t(round(Classification_Table,2)), max_data_report))
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))
tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)
large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)
Importance_table <- cbind(log_importance,tree_importance_final,large_tree_importance_final)
colnames(Importance_table) <- c("Logistic Regression", "CART 1", "CART 2")
rownames(Importance_table) <- rownames(log_importance)
## printing the result in a clean-slate table
knitr::kable(round(Importance_table,2))
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_log,
validation_prediction_class_tree,
validation_prediction_class_tree_large)
validation_hit_rates = rbind(
100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual),
100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual),
100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual)
)
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("Logistic Regression", "First CART", "Second CART")
knitr::kable(validation_hit_rates)
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_log,
estimation_prediction_class_tree,
estimation_prediction_class_tree_large)
estimation_hit_rates = rbind(
100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual),
100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual),
100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual)
)
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(estimation_hit_rates)
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(validation_prediction_best*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!validation_prediction_best)*validation_data[,dependent_variable])/sum(validation_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!validation_prediction_best)*(!validation_data[,dependent_variable]))/sum((!validation_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)
colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
validation_actual_class <- as.numeric(validation_data[,dependent_variable])
pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)
test1<-performance(pred_tree, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive rate CART 1")
plot1 <- ggplot(df1, aes(x=`False Positive rate CART 1`, y=`True Positive rate CART 1`)) + geom_line()
test2<-performance(pred_log, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate log reg", "True Positive rate log reg")
plot2 <- ggplot(df2, aes(x=`False Positive rate log reg`, y=`True Positive rate log reg`)) + geom_line()
test3<-performance(pred_tree_large, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate CART 2", "True Positive rate CART 2")
plot3 <- ggplot(df3, aes(x=`False Positive rate CART 2`, y=`True Positive rate CART 2`)) + geom_line()
# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
df <- melt(df, id=1)
df$variable <- sub("True Positive rate ", "", df$variable)
colnames(df)[1] <- "False Positive rate"
df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
validation_actual <- validation_data[,dependent_variable]
all1s <- sum(validation_actual);
probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame1 <- data.frame(
`CART 1 % of validation data` = res[1,],
`CART 1 % of class 1` = res[2,],
check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % of validation data`, y=`CART 1 % of class 1`)) + geom_line()
probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame2 <- data.frame(
`CART 2 % of validation data` = res[1,],
`CART 2 % of class 1` = res[2,],
check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % of validation data`, y=`CART 2 % of class 1`)) + geom_line()
probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(validation_actual), sum(validation_actual[useonly])/all1s)
}))
frame3 <- data.frame(
`log reg % of validation data` = res[1,],
`log reg % of class 1` = res[2,],
check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % of validation data`, y=`log reg % of class 1`)) + geom_line()
# We can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we are going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" % of class 1", "", df$variable)
colnames(df)[1] <- "% of validation data selected"
df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
knitr::kable(Profit_Matrix)
actual_class <- validation_data[,dependent_variable]
probs <- validation_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame1 <- data.frame(
`CART 1 % selected` = res[1,],
`CART 1 est. profit` = res[2,],
check.names = FALSE
)
plot1 <- ggplot(frame1, aes(x=`CART 1 % selected`, y=`CART 1 est. profit`)) + geom_line()
probs <- validation_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame2 <- data.frame(
`CART 2 % selected` = res[1,],
`CART 2 est. profit` = res[2,],
check.names = FALSE
)
plot2 <- ggplot(frame2, aes(x=`CART 2 % selected`, y=`CART 2 est. profit`)) + geom_line()
probs <- validation_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame3 <- data.frame(
`log reg % selected` = res[1,],
`log reg est. profit` = res[2,],
check.names = FALSE
)
plot3 <- ggplot(frame3, aes(x=`log reg % selected`, y=`log reg est. profit`)) + geom_line()
# we can plot the curves individually
# grid.arrange(plot1, plot2, plot3)   # use `fig.height=7.5` for the grid plot
# But we're going to combine them instead
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" est. profit", "", df$variable)
colnames(df)[1] <- "% of validation data selected"
df
}))
ggplot(df.all, aes(x=`% of validation data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
######for train data#####
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_log,
test_prediction_class_tree,
test_prediction_class_tree_large
)
test_hit_rates = rbind(
100*sum(test_prediction_class_log==test_actual)/length(test_actual),
100*sum(test_prediction_class_tree==test_actual)/length(test_actual),
100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual)
)
colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- c("Logistic Regression","First CART", "Second CART")
knitr::kable(test_hit_rates)
test_prediction_best = test_predictions[which.max(test_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)
colnames(conf_matrix) <- c(paste("Predicted 1 (", class_1_interpretation, ")", sep = ""), paste("Predicted 0 (", class_0_interpretation, ")", sep = ""))
rownames(conf_matrix) <- c(paste("Actual 1 (", class_1_interpretation, ")", sep = ""), paste("Actual 0 (", class_0_interpretation, ")", sep = ""))
knitr::kable(conf_matrix)
test_actual_class <- as.numeric(test_data[,dependent_variable])
pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)
test<-performance(pred_tree_test, "tpr", "fpr")
df1<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df1) <- c("False Positive rate CART 1", "True Positive CART 1")
test2<-performance(pred_tree_large_test, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART 2", "True Positive CART 2")
test3<-performance(pred_log_test, "tpr", "fpr")
df3<- cbind(as.data.frame(test3@x.values),as.data.frame(test3@y.values))
colnames(df3) <- c("False Positive rate log reg", "True Positive log reg")
df.all <- do.call(rbind, lapply(list(df1, df2, df3), function(df) {
df <- melt(df, id=1)
df$variable <- sub("True Positive ", "", df$variable)
colnames(df)[1] <- "False Positive rate"
df
}))
ggplot(df.all, aes(x=`False Positive rate`, y=value, colour=variable)) + geom_line() + ylab("True Positive rate") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
test_actual <- test_data[,dependent_variable]
all1s <- sum(test_actual)
probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame1 <- data.frame(
`CART 1 % of validation data` = res[1,],
`CART 1 % of class 1` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame2 <- data.frame(
`CART 2 % of validation data` = res[1,],
`CART 2 % of class 1` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- 100*Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s)
}))
frame3 <- data.frame(
`log reg % of validation data` = res[1,],
`log reg % of class 1` = res[2,],
check.names = FALSE
)
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" % of class 1", "", df$variable)
colnames(df)[1] <- "% of test data selected"
df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("% of class 1 captured") + geom_abline(intercept = 0, slope = 1,linetype="dotted",colour="green")
actual_class<- test_data[,dependent_variable]
probs <- test_Probability_class1_tree
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame1 <- data.frame(
`CART 1 % selected` = res[1,],
`CART 1 est. profit` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_tree_large
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame2 <- data.frame(
`CART 2 % selected` = res[1,],
`CART 2 est. profit` = res[2,],
check.names = FALSE
)
probs <- test_Probability_class1_log
xaxis <- sort(unique(c(0,1,probs)), decreasing = TRUE)
res <- Reduce(cbind,lapply(xaxis, function(prob){
useonly <- which(probs >= prob)
predict_class <- 1*(probs >= prob)
theprofit <- Profit_Matrix[1,1]*sum(actual_class==1 & predict_class==1)+
Profit_Matrix[1,2]*sum(actual_class==1 & predict_class==0)+
Profit_Matrix[2,1]*sum(actual_class==0 & predict_class==1)+
Profit_Matrix[2,2]*sum(actual_class==0 & predict_class==0)
c(100*length(useonly)/length(actual_class), theprofit)
}))
frame3 <- data.frame(
`log reg % selected` = res[1,],
`log reg est. profit` = res[2,],
check.names = FALSE
)
df.all <- do.call(rbind, lapply(list(frame1, frame2, frame3), function(df) {
df <- melt(df, id=1)
df$variable <- sub(" est. profit", "", df$variable)
colnames(df)[1] <- "% of test data selected"
df
}))
ggplot(df.all, aes(x=`% of test data selected`, y=value, colour=variable)) + geom_line() + ylab("Estimated profit")
rmarkdown::render()
rmarkdown::render(input = "Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render(input = "./Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render(input = "../Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render("../Sesiones/Sesiones_5_6Clasificacion/Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6Clasificacion")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6Clasificacion/")
rmarkdown::render("../Sesiones/Sesiones_5_6_Clasificacion/Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
getwd()
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render(Deafult_Tarjeta_Credito.Rmd)
rmarkdown::render('Deafult_Tarjeta_Credito.Rmd')
rmarkdown::render("../Deafult_Tarjeta_Credito.Rmd")
rmarkdown::render("./Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics")
rmarkdown::render("../Sesiones/Sesiones_5_6_Clasificacion/Deafault_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
rmarkdown::render("Deafault_Tarjeta_Credito.Rmd")
list.files()
rmarkdown::render("Default_Tarjeta_Credito.Rmd")
getwd()
knit("Default_Tarjeta_Credito.Rmd")
knit2html("Default_Tarjeta_Credito.Rmd")
version
getOption(“rstudio.markdownToHTML”)
getOption()
make_pdf_file = 0 # Haga este número 1 si quiere un archivo PDF, 0 para HTML
source("../../AnalyticsLibraries/library.R")
getOption(“rstudio.markdownToHTML”)
getOption("rstudio.markdownToHTML")
getwd()
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),
sum(estimation_data[,dependent_variable]==0)),
nrow=1)
########################   NO TOCAR    #############################
make_pdf_file = 0 # DEJE 0 SI PARA PRODUCIR UN ARCHIVO HTML, 1 PARA UN PDF
local_directory = "."
source(paste(local_directory,"../../AnalyticsLibraries/library.R", sep="/"))
source(paste(local_directory,"../../AnalyticsLibraries/heatmapOutput.R", sep = "/"))
# Package options
ggthemr('fresh')  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.2)
options(knitr.kable.NA = '')
dformat <-function(df) {
if (class(df) != "data.frame")
df <- as.data.frame(df)
x <- lapply(colnames(df), function(col) {
if (is.numeric(df[, col]))
normalize_bar(rgb(238, 238, 238, max=255), min=0.1, na.rm=TRUE)
else
formatter("span")
})
names(x) <- colnames(df)
formattable(df, x)
}
if (make_pdf_file) {
dformat<- function(df) knitr::kable(df)
}
# SET UP
# When running the case on a local computer, modify this in case you saved the case in a different directory
# (e.g. local_directory <- "C:/user/MyDocuments" )
# type in the Console below help(getwd) and help(setwd) for more information
#local_directory <- paste(getwd(),"CourseSessions/Sessions67", sep="/")
#local_directory <- "~INSEADAnalytics/CourseSessions/Sessions67"
# Please ENTER the filename that indicates subsets of the data to use (e.g. only a specific cluster)
# This file need to have 2 columns with the second one indicating the cluster ID of the observation.
# The rows of this files are aligned with those of the datafile_name one
# This is used ONLY for the report "MyBoatsDrivers"
cluster_file_ini = "Boats_cluster" # make sure this file exists in the "data" directory
datafile_name = "Boats"
# Please ENTER the class (dependent) variable:
# Please use numbers, not column names! e.g. 82 uses the 82nd column are dependent variable.
# YOU NEED TO MAKE SURE THAT THE DEPENDENT VARIABLES TAKES ONLY 2 VALUES: 0 and 1!!!
dependent_variable= 82
# Please ENTER the attributes to use as independent variables
# Please use numbers, not column names! e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
independent_variables= c(54:80) # use 54-80 for boats
# Please ENTER the profit/cost values for the correctly and wrong classified data:
actual_1_predict_1 = 100
actual_1_predict_0 = -75
actual_0_predict_1 = -50
actual_0_predict_0 = 0
# Please ENTER the probability threshold above which an observations
# is predicted as class 1:
Probability_Threshold=50 # between 1 and 99%
# Please ENTER the percentage of data used for estimation
estimation_data_percent = 80
validation_data_percent = 10
# Please enter 0 if you want to "randomly" split the data in estimation and validation/test
random_sampling = 0
# Tree parameter
# PLEASE ENTER THE Tree (CART) complexity control cp (e.g. 0.001 to 0.02, depending on the data)
CART_cp = 0.01
# Please enter the minimum size of a segment for the analysis to be done only for that segment
min_segment = 100
# Please enter the maximum number of observations to show in the report and slides
# (DEFAULT is 50. If the number is large the report and slides may not be generated - very slow or will crash!!)
max_data_report = 10 # can also chance in server.R
Probability_Threshold = Probability_Threshold/100 # make it between 0 and 1
ProjectData <- read.csv(paste(paste(local_directory, "data", sep="/"), paste(datafile_name,"csv", sep="."), sep = "/")) # this contains only the matrix ProjectData
ProjectData=data.matrix(ProjectData)
# if (datafile_name == "Boats")
#   colnames(ProjectData)<-gsub("\\."," ",colnames(ProjectData))
dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectData), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectData), max(i,1))))
if (length(unique(ProjectData[,dependent_variable])) !=2){
cat("\n*****\n BE CAREFUL, THE DEPENDENT VARIABLE TAKES MORE THAN 2 VALUES...")
cat("\nSplitting it around its median...\n*****\n ")
new_dependent = ProjectData[,dependent_variable] >= median(ProjectData[,dependent_variable])
ProjectData[,dependent_variable] <- 1*new_dependent
}
Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix)<- c("Predict 1", "Predict 0")
rownames(Profit_Matrix) <- c("Actual 1", "Actual 0")
test_data_percent = 100-estimation_data_percent-validation_data_percent
CART_control = rpart.control(cp = CART_cp)
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
rmarkdown::render("Deafult_Tarjeta_Credito.Rmd")
setwd("C:/Users/eduardo_canas/Documents/R/UCAanalytics/Sesiones/Sesiones_5_6_Clasificacion/")
libraries_used=c("devtools","knitr","graphics","grDevices","xtable","pryr",
"Hmisc")
remove.packages(pkgs = libraries_used)
1+1
3*5+12
knitr::opts_chunk$set(echo = TRUE)
1+1
a <-  5
b<- 31
c <- a*b-100
c
1+1
a <-  5
b<- 31
c <- a*b-100
c
