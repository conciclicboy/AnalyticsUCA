"bedrooms","room_type")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmse")
View(X_train)
# One Hot Encoding
vecindarios<-model.matrix(~neighborhood-1,head(b))
tipos_cuarto<-model.matrix(~room_type-1,head(b))
b <- cbind(b, vecindarios,tipos_cuarto)
b<-read.csv("./data/tomslee_airbnb_amsterdam_1476_2017-07-22.csv")
#b$var1 <- NULL
#b$var2 <- NULL
#b$var3 <- NULL
#b$var4 <- NULL
b$country <- NULL
b$borough <- NULL
b$bathrooms <- NULL
b$minstay <- NULL
colnames(b)
#filtrando data frame entre percentiles 1 y 99 de la variable precio
b<- b [(b$price>=quantile(b$price,0.01)) & (b$price <= quantile(b$price,0.99)),]
#Graficando nuevamente
pl <- ggplot(b,aes(x=price))
pl+geom_histogram(fill='blue',alpha=0.5)+theme_minimal()
# One Hot Encoding
vecindarios<-model.matrix(~neighborhood-1,head(b))
tipos_cuarto<-model.matrix(~room_type-1,head(b))
b <- cbind(b, vecindarios,tipos_cuarto)
# Please ENTER the percentage of data used for estimation
porcion_train = 0.9
porcion_test = 0.1
# Librería dplyr
library(dplyr)
# Creando índice con función "mutate"" de dplyr
# Esto lo utilizaremos solo como referencia
b <- mutate(b,id = row_number())
# Creando datos de entrenamiento con función "sample_frac" de dplyr
# La función toma una muestra aleatoria de tamaño "porcion_train" de "b"
train_set <- sample_frac(b,porcion_train)
# Creando datos de evaluación con función "anti_join" de dplyr
# La función extraen los datos de "b" que no coinciden con "train_set"
# usando como referencia la columna "id"
test_set <- anti_join(b, train_set, by = 'id')
#,results='asis'
# Creando modelo
model_1<- lm(formula = price ~ room_type + neighborhood +
reviews +overall_satisfaction + accommodates +
bedrooms,data = train_set)
# Reportando modelo: usted solo necesita poner "summary(model)"
# Eso le entrega los resultados del modelo pero se imprime en formato feo
# Nosotros lo vamos a imprimir como una "xtable" en "knitr::kable"
library(xtable)
knitr::kable(xtable(summary(model_1)))
# Creando modelo
model_2<-lm(formula = log(price) ~ room_type + neighborhood +
reviews +overall_satisfaction + log(accommodates) +
bedrooms,data = train_set)
# Reportando modelo
knitr::kable(xtable(summary(model_2)))
library(rpart)
model_3 <- rpart(price ~ room_type + neighborhood +
reviews +overall_satisfaction + accommodates +
bedrooms,data = train_set, method="anova")
library(xgboost)
#Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"room_type")]
X_test<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"room_type")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmse")
View(X_train)
#Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
X_test<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmse")
predict(model_4,test_set)
model_4 = predict(model_4,X_test)
model_4 = predict(model_4,as.matrix(X_test))
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmse")
mae(test_set$price, predict(model_4,as.matrix(X_test)))
library(Metrics)
mae(test_set$price, predict(model_4,as.matrix(X_test)))
pred_4<- predict(model_4,as.matrix(X_test))
X_test<- test[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
#Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
X_test<- test[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
X_test<- test_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
mae1
mae2
mae3
mae43
mae4
mse
mse1<-mse(test_set$price,predict(model_1,test_set))
mse2<-mse(test_set$price,exp(predict(model_2,test_set)))
mse3<-mse(test_set$price,predict(model_3,test_set))
mse4<-mse(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mse1
mse2
mse3
mse4
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 50, objective = "reg:linear",
eval_metric="rmae")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
mae(train_set$price,predict(model_1,train_set))
mae(train_set$price,predict(model_2,train_set))
mae(train_set$price,predict(model_3,train_set))
mae(as.matrix(y_train),predict(model_4,as.matrix(X_train)))
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 2, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = .01, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = .1, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.5, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 5, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 6, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 10, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 1.2, nthread = 6, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 6,
eta = 1.2, nthread = 6, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 6, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
mae1
mae2
mae3
mae4
pred_1<-as.matrix(pred_1)
View(pred_1)
pred_2<-as.matrix(pred_2)
pred_3<-as.matrix(pred_3)
pred_4<-as.matrix(pred_4)
View(pred_1)
View(pred_2)
View(pred_3)
View(pred_4)
#Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
X_test<- test_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
#Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
X_test<- test_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
#Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1.2, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 1, nthread = 2, nrounds = 300, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
mae3
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.1, nthread = 2, nrounds = 300, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
mae1
mae2
mae3
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.3, nthread = 2, nrounds = 300, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 3,
eta = 0.3, nthread = 2, nrounds = 1000, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 2,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
mae4<-mae(as.matrix(y_test),predict(model_4,as.matrix(X_test)))
mae4
# Sacando predicciones
pred_train_1=predict(model_1,train_set)
pred_test_1=predict(model_1,test_set)
# Creando modelo
model_2<-lm(formula = log(price) ~ room_type + neighborhood +
reviews +overall_satisfaction + log(accommodates) +
bedrooms,data = train_set)
# Sacando predicciones
pred_train_2=exp(predict(model_2,train_set))
pred_test_2=exp(predict(model_2,test_set))
# Reportando modelo
knitr::kable(xtable(summary(model_2)))
model_3 <- rpart(price ~ room_type + neighborhood +
reviews +overall_satisfaction + accommodates +
bedrooms,data = train_set, method="anova")
# Sacando predicciones
pred_train_3=predict(model_3,train_set)#entrenamiento
pred_test_3=predict(model_3,test_set)#evaluación
# Predicciones
pred_train_4<-predict(model_4,as.matrix(X_train))
pred_test_4<-predict(model_4,as.matrix(X_test))
mae1<-mae(test_set$price,pred_test_1)
mae2<-mae(test_set$price,pred_test_2)
mae3<-mae(test_set$price,pred_test_3)
mae4<-mae(test_set$price,pred_test_4)
mae1
mae2
mae3
mae4
mean(b$price)
sd(b$price)
mean(pred_train_1)
sd(pred_train_1)
sd(pred_train_3)
sd(pred_train_4)
mean(pred_train_2)
mean(pred_train_3)
mean(pred_train_43)
mean(pred_train_4)
mean(pred_train_1)
mean(pred_train_21)
mean(pred_train_1)
mean(pred_train_2)
mean(pred_train_3)
mean(pred_train_4)
# Primero vamos a añadir a X_train y X_test los resultados del modelo lineal
xx<-cbind(X_train, as.matrix(pred_train_1))
View(xx)
# Primero vamos a añadir a X_train y X_test los resultados del modelo lineal
X_train_2<-cbind(X_train, as.matrix(pred_train_1),as.matrix(pred_train_2))
X_test_2<-cbind(X_test, as.matrix(pred_test_1),as.matrix(pred_test_2))
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
colnames(X_train_2)
colnames(X_test_2)
colnames(X_train)
names(X_train)
names(X_train)[32]
names(X_train_2)[32]
?as.array()
y_1<-as.matrix(pred_train_1)
View(y_1)
# Primero vamos a añadir a X_train y X_test los resultados del modelo lineal
y_1<- as.matrix(pred_train_1)
y_2<- as.matrix(pred_train_2)
X_train_2<-cbind(X_train,y_1,y_2)
y_1<- as.matrix(pred_test_1)
y_2<- as.matrix(pred_test_2)
X_test_2<-cbind(X_test, y_1,y_2)
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Error Medio
mae5<-mae(test_set$price,pred_test_5)
mae5
mae1
mae2
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.5, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.5, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Error Medio
mae5<-mae(test_set$price,pred_test_5)
mae5
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.75, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.75, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
mae5
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 1.05, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Error Medio
mae5<-mae(test_set$price,pred_test_5)
mae5
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 1, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 1, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Error Medio
mae5<-mae(test_set$price,pred_test_5)
mae5
# Modelo
model_5 <- xgboost(data = as.matrix(X_train_2), label = as.matrix(y_train), max.depth = 4,
eta = 0.5, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_5<-predict(model_5,as.matrix(X_train_2))
pred_test_5<-predict(model_5,as.matrix(X_test_2))
# Error Medio
mae5<-mae(test_set$price,pred_test_5)
mae5
# Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
X_test<- test_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
# Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_4<-predict(model_4,as.matrix(X_train))
# Predicciones
pred_train_4<-predict(model_4,as.matrix(X_train))
pred_test_4<-predict(model_4,as.matrix(X_test))
### Modelo 4: XGBoost
```{r,message=FALSE, warning=FALSE,eval=FALSE}
library(xgboost)
# Primero vamos a definir las matrices de entrenamiento y evaluación
X_train<- train_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
X_test<- test_set[,c(colnames(vecindarios),colnames(tipos_cuarto),"reviews",
"overall_satisfaction", "accommodates","bedrooms",
"longitude", "latitude")]
# Luego definimos los vectores de la variable dependiente
y_train<- train_set$price
y_test<-test_set$price
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Modelo
model_4 <- xgboost(data = as.matrix(X_train), label = as.matrix(y_train), max.depth = 4,
eta = 0.3, nthread = 2, nrounds = 100, objective = "reg:linear",
eval_metric="rmse")
# Predicciones
pred_train_4<-predict(model_4,as.matrix(X_train))
print("Modelo 1:",mae1)
print("Modelo 1:" & mae1)
print((paste("Modelo 1:" & mae1))
print((paste("Modelo 1:" & mae1)))
print((paste("Modelo 1:", mae1)))
print((paste("Modelo 2:", mae2)))
mae5
mae5/sum(y_test)
y_test
mae5/mean(y_test)
mae1/mean(y_test)
mae2/mean(y_test)
mae3/mean(y_test)
mae4/mean(y_test)
